{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rank-1 CNN on MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-75f115764713>:13: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./mnist/data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./mnist/data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting ./mnist/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import re\n",
    "import os\n",
    "\n",
    "#import MNIST #\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "#save the MNIST data in to the folder /mnist/datat/\n",
    "mnist = input_data.read_data_sets(\"./mnist/data/\", one_hot=True)\n",
    "\n",
    "# to plot images import the necessary libraries        \n",
    "#%pylab      \n",
    "\n",
    "# use matplotlib as inline\n",
    "%matplotlib inline \n",
    "\n",
    "from matplotlib.pyplot import *\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allocator_type ='BFC'\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# conv1_1\n",
    "x = tf.placeholder(\"float\", [None, 28, 28, 1])\n",
    "phase = tf.placeholder(tf.bool, name='phase') #batch normalization을 training모드인지 test모드인지\n",
    "\n",
    "input_num = 1 #number of input channels\n",
    "dim = 3 # Size of the 1-D vectors\n",
    "num = 8 # Number of P and Q vectors. The combination of P and Q vectors becomes num * num\n",
    "output_num = num*num  # number of filters = number of outputs\n",
    "W_list1_1 = [] # List for W\n",
    "\n",
    "P1_1 = tf.Variable(tf.random_uniform([num,dim], -0.5, 0.5))\n",
    "Q1_1 = tf.Variable(tf.random_uniform([num,dim], -0.5, 0.5))\n",
    "T1_1 = tf.Variable(tf.random_uniform([output_num,input_num,1,1], -0.5, 0.5))\n",
    "\n",
    "k=0\n",
    "for i in range(num): \n",
    "    for j in range(num):\n",
    "      P1_1_temp = tf.reshape(P1_1[i], [dim, 1])\n",
    "      Q1_1_temp = tf.reshape(Q1_1[j], [1, dim])\n",
    "      Mul_temp1_1_a = tf.matmul(P1_1_temp, Q1_1_temp)\n",
    "      Mul_temp1_1_b = tf.reshape(Mul_temp1_1_a, [1, Mul_temp1_1_a.shape[0], Mul_temp1_1_a.shape[1]])\n",
    "      Mul_temp1_1_c = T1_1[k]*Mul_temp1_1_b\n",
    "      k = k + 1\n",
    "      W_list1_1.append(Mul_temp1_1_c)\n",
    "\n",
    "W1_1 = tf.transpose(tf.stack(W_list1_1, axis=0))\n",
    "L1 = tf.nn.conv2d(x, W1_1, strides=[1, 1, 1, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# conv1_2\n",
    "\n",
    "input_num = 64 #number of input channels\n",
    "dim = 3 # Size of the 1-D vectors\n",
    "num = 8 # Number of P and Q vectors. The combination of P and Q vectors becomes num * num\n",
    "output_num = num*num  # number of filters = number of outputs\n",
    "W_list1_2 = [] # List for W\n",
    "\n",
    "P1_2 = tf.Variable(tf.random_uniform([num,dim], -0.5, 0.5))\n",
    "Q1_2 = tf.Variable(tf.random_uniform([num,dim], -0.5, 0.5))\n",
    "T1_2 = tf.Variable(tf.random_uniform([output_num,input_num,1,1], -0.5, 0.5))\n",
    "\n",
    "k=0\n",
    "for i in range(num): \n",
    "    for j in range(num):\n",
    "      P1_2_temp = tf.reshape(P1_2[i], [dim, 1])\n",
    "      Q1_2_temp = tf.reshape(Q1_2[j], [1, dim])\n",
    "      Mul_temp1_2_a = tf.matmul(P1_2_temp, Q1_2_temp)\n",
    "      Mul_temp1_2_b = tf.reshape(Mul_temp1_2_a, [1, Mul_temp1_2_a.shape[0], Mul_temp1_2_a.shape[1]])\n",
    "      Mul_temp1_2_c = T1_2[k]*Mul_temp1_2_b\n",
    "      k = k + 1\n",
    "      W_list1_2.append(Mul_temp1_2_c)\n",
    "\n",
    "W1_2 = tf.transpose(tf.stack(W_list1_2, axis=0))\n",
    "L1 = tf.nn.conv2d(L1, W1_2, strides=[1, 1, 1, 1], padding='SAME')\n",
    "L1 = tf.nn.max_pool(L1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# conv2_1\n",
    "\n",
    "input_num = 64 #number of input channels\n",
    "dim = 3   # Size of the 1-D vectors\n",
    "num = 12  # Number of P and Q vectors. The combination of P and Q vectors becomes num * num\n",
    "output_num = num*num  # number of filters = number of outputs\n",
    "W_list2_1 = [] # List for W\n",
    "\n",
    "P2_1 = tf.Variable(tf.random_uniform([num,dim], -0.5, 0.5))\n",
    "Q2_1 = tf.Variable(tf.random_uniform([num,dim], -0.5, 0.5))\n",
    "\n",
    "T2_1 = tf.Variable(tf.random_uniform([output_num,input_num,1,1], -0.5, 0.5))\n",
    "k=0\n",
    "for i in range(num): \n",
    "    for j in range(num):\n",
    "      #P_transpose = tf.reshape(P[:,0], [dim, 1]) --> 이렇게 해도 되지만....\n",
    "      #P는 row가 column이다라고 기억하고서 다음과 같이 하자...\n",
    "      P2_1_temp = tf.reshape(P2_1[i], [dim, 1])\n",
    "      Q2_1_temp = tf.reshape(Q2_1[j], [1, dim])\n",
    "      Mul_temp2_1_a = tf.matmul(P2_1_temp, Q2_1_temp)\n",
    "      Mul_temp2_1_b = tf.reshape(Mul_temp2_1_a, [1, Mul_temp2_1_a.shape[0], Mul_temp2_1_a.shape[1]])\n",
    "      Mul_temp2_1_c = T2_1[k]*Mul_temp2_1_b\n",
    "      k = k + 1\n",
    "      W_list2_1.append(Mul_temp2_1_c)\n",
    "\n",
    "W2_1 = tf.transpose(tf.stack(W_list2_1, axis=0))\n",
    "L2 = tf.nn.conv2d(L1, W2_1, strides=[1, 1, 1, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# conv2_2\n",
    "\n",
    "input_num = 144  #number of input channels\n",
    "dim = 3   # Size of the 1-D vectors\n",
    "num = 12  # Number of P and Q vectors. The combination of P and Q vectors becomes num * num\n",
    "output_num = num*num  # number of filters = number of outputs\n",
    "W_list2_2 = [] # List for W\n",
    "\n",
    "P2_2 = tf.Variable(tf.random_uniform([num,dim], -0.5, 0.5))\n",
    "Q2_2 = tf.Variable(tf.random_uniform([num,dim], -0.5, 0.5))\n",
    "T2_2 = tf.Variable(tf.random_uniform([output_num,input_num,1,1], -0.5, 0.5))\n",
    "\n",
    "k=0\n",
    "for i in range(num): \n",
    "    for j in range(num):\n",
    "      P2_2_temp = tf.reshape(P2_2[i], [dim, 1])\n",
    "      Q2_2_temp = tf.reshape(Q2_2[j], [1, dim])\n",
    "      Mul_temp2_2_a = tf.matmul(P2_2_temp, Q2_2_temp)\n",
    "      Mul_temp2_2_b = tf.reshape(Mul_temp2_2_a, [1, Mul_temp2_2_a.shape[0], Mul_temp2_2_a.shape[1]])\n",
    "      Mul_temp2_2_c = T2_2[k]*Mul_temp2_2_b\n",
    "      k = k + 1\n",
    "      W_list2_2.append(Mul_temp2_2_c)\n",
    "\n",
    "W2_2 = tf.transpose(tf.stack(W_list2_2, axis=0))\n",
    "L2 = tf.nn.conv2d(L2, W2_2, strides=[1, 1, 1, 1], padding='SAME')\n",
    "L2 = tf.nn.max_pool(L2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# conv3_1\n",
    "\n",
    "input_num = 144  #number of input channels\n",
    "dim = 3    # Size of the 1-D vectors\n",
    "num = 16   # Number of P and Q vectors. The combination of P and Q vectors becomes num * num\n",
    "output_num = num*num  # number of filters = number of outputs\n",
    "W_list3_1 = [] # List for W\n",
    "\n",
    "P3_1 = tf.Variable(tf.random_uniform([num,dim], -0.5, 0.5))\n",
    "Q3_1 = tf.Variable(tf.random_uniform([num,dim], -0.5, 0.5))\n",
    "T3_1 = tf.Variable(tf.random_uniform([output_num,input_num,1,1], -0.5, 0.5))\n",
    "\n",
    "k=0\n",
    "for i in range(num): \n",
    "    for j in range(num):\n",
    "      P3_1_temp = tf.reshape(P3_1[i], [dim, 1])\n",
    "      Q3_1_temp = tf.reshape(Q3_1[j], [1, dim])\n",
    "      Mul_temp3_1_a = tf.matmul(P3_1_temp, Q3_1_temp)\n",
    "      Mul_temp3_1_b = tf.reshape(Mul_temp3_1_a, [1, Mul_temp3_1_a.shape[0], Mul_temp3_1_a.shape[1]])\n",
    "      Mul_temp3_1_c = T3_1[k]*Mul_temp3_1_b\n",
    "      k = k + 1\n",
    "      W_list3_1.append(Mul_temp3_1_c)\n",
    "\n",
    "W3_1 = tf.transpose(tf.stack(W_list3_1, axis=0))\n",
    "L3 = tf.nn.conv2d(L2, W3_1, strides=[1, 1, 1, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# conv3_2\n",
    "\n",
    "input_num = 256   #number of input channels\n",
    "dim = 3   # Size of the 1-D vectors\n",
    "num = 16  # Number of P and Q vectors. The combination of P and Q vectors becomes num * num\n",
    "output_num = num*num  # number of filters = number of outputs\n",
    "W_list3_2 = [] # List for W\n",
    "\n",
    "P3_2 = tf.Variable(tf.random_uniform([num,dim], -0.5, 0.5))\n",
    "Q3_2 = tf.Variable(tf.random_uniform([num,dim], -0.5, 0.5))\n",
    "T3_2 = tf.Variable(tf.random_uniform([output_num,input_num,1,1], -0.5, 0.5))\n",
    "\n",
    "k=0\n",
    "for i in range(num): \n",
    "    for j in range(num):\n",
    "      P3_2_temp = tf.reshape(P3_2[i], [dim, 1])\n",
    "      Q3_2_temp = tf.reshape(Q3_2[j], [1, dim])\n",
    "      Mul_temp3_2_a = tf.matmul(P3_2_temp, Q3_2_temp)\n",
    "      Mul_temp3_2_b = tf.reshape(Mul_temp3_2_a, [1, Mul_temp3_2_a.shape[0], Mul_temp3_2_a.shape[1]])\n",
    "      Mul_temp3_2_c = T3_2[k]*Mul_temp3_2_b\n",
    "      k = k + 1\n",
    "      W_list3_2.append(Mul_temp3_2_c)\n",
    "\n",
    "W3_2 = tf.transpose(tf.stack(W_list3_2, axis=0))\n",
    "L3 = tf.nn.conv2d(L3, W3_2, strides=[1, 1, 1, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# conv3_3\n",
    "\n",
    "input_num = 256  #number of input channels\n",
    "dim = 3   # Size of the 1-D vectors\n",
    "num = 16  # Number of P and Q vectors. The combination of P and Q vectors becomes num * num\n",
    "output_num = num*num  # number of filters = number of outputs\n",
    "W_list3_3 = [] # List for W\n",
    "\n",
    "P3_3 = tf.Variable(tf.random_uniform([num,dim], -0.5, 0.5))\n",
    "Q3_3 = tf.Variable(tf.random_uniform([num,dim], -0.5, 0.5))\n",
    "T3_3 = tf.Variable(tf.random_uniform([output_num,input_num,1,1], -0.5, 0.5))\n",
    "\n",
    "k=0\n",
    "for i in range(num): \n",
    "    for j in range(num):\n",
    "      P3_3_temp = tf.reshape(P3_3[i], [dim, 1])\n",
    "      Q3_3_temp = tf.reshape(Q3_3[j], [1, dim])\n",
    "      Mul_temp3_3_a = tf.matmul(P3_3_temp, Q3_3_temp)\n",
    "      Mul_temp3_3_b = tf.reshape(Mul_temp3_3_a, [1, Mul_temp3_3_a.shape[0], Mul_temp3_3_a.shape[1]])\n",
    "      Mul_temp3_3_c = T3_3[k]*Mul_temp3_3_b\n",
    "      k = k + 1\n",
    "      W_list3_3.append(Mul_temp3_3_c)\n",
    "\n",
    "W3_3 = tf.transpose(tf.stack(W_list3_3, axis=0))\n",
    "L3 = tf.nn.conv2d(L3, W3_3, strides=[1, 1, 1, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dense(inputs, units, name=None): \n",
    "#\"\"\"3x3 conv layer: ReLU + He initialization\"\"\" \n",
    "# He initialization: normal dist with stdev = sqrt(2.0/fan-in) \n",
    "    stddev = np.sqrt(2 / int(inputs.shape[1])) \n",
    "    out = tf.layers.dense(inputs, units, \n",
    "                           kernel_initializer=tf.random_normal_initializer(stddev=stddev), \n",
    "                           kernel_regularizer=tf.contrib.layers.l2_regularizer(1.0), \n",
    "                           name=name) \n",
    "\n",
    "    return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dense_relu(inputs, units, name=None): \n",
    "# \"\"\"3x3 conv layer: ReLU + He initialization\"\"\" \n",
    " \n",
    " \n",
    "# He initialization: normal dist with stdev = sqrt(2.0/fan-in) \n",
    "    stddev = np.sqrt(2 / int(inputs.shape[1])) \n",
    "    out = tf.layers.dense(inputs, units, activation=tf.nn.relu, \n",
    "                         kernel_initializer=tf.random_normal_initializer(stddev=stddev), \n",
    "                         kernel_regularizer=tf.contrib.layers.l2_regularizer(1.0), \n",
    "                         name=name) \n",
    " \n",
    " \n",
    "#   tf.summary.histogram('act' + name, out) \n",
    "\n",
    " \n",
    "    return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dense_batch_relu(x, output, phase, scope):\n",
    "    with tf.variable_scope(scope):\n",
    "        h1 = tf.contrib.layers.fully_connected(x, output, \n",
    "                                               activation_fn=None,\n",
    "                                               scope='dense')\n",
    "        h2 = tf.contrib.layers.batch_norm(h1, \n",
    "                                          center=True, scale=True, \n",
    "                                          is_training=phase,\n",
    "                                          scope='bn')\n",
    "        return tf.nn.relu(h2, 'relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_ = tf.placeholder(\"float\", [None,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fully connected layers\n",
    "out = tf.contrib.layers.flatten(L3)\n",
    "out = dense_batch_relu(out, 2048, phase,'fc1')\n",
    "out = tf.nn.dropout(out, 0.5) \n",
    "out = dense_batch_relu(out, 1024, phase,'fc2')\n",
    "out = tf.nn.dropout(out, 0.5) \n",
    "\n",
    "class_number = 10\n",
    "logits = dense(out, class_number, 'fc3')  # class number =2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#########\n",
    "# Result\n",
    "######\n",
    "is_correct = tf.equal(tf.argmax(logits, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-35-5ef564aa7215>:6: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "Epoch: 0001 Avg. cost = 0.287\n",
      "--- 116.9731342792511 seconds ---\n",
      "Accuracy: 0.35869999960064886\n",
      "Epoch: 0002 Avg. cost = 0.119\n",
      "--- 99.87699961662292 seconds ---\n",
      "Accuracy: 0.44809999942779544\n",
      "Epoch: 0003 Avg. cost = 0.092\n",
      "--- 100.00199508666992 seconds ---\n",
      "Accuracy: 0.5119999957084655\n",
      "Epoch: 0004 Avg. cost = 0.077\n",
      "--- 99.8930037021637 seconds ---\n",
      "Accuracy: 0.6080000001192093\n",
      "Epoch: 0005 Avg. cost = 0.068\n",
      "--- 100.19200325012207 seconds ---\n",
      "Accuracy: 0.7151000028848649\n",
      "Epoch: 0006 Avg. cost = 0.059\n",
      "--- 100.26400899887085 seconds ---\n",
      "Accuracy: 0.869699998497963\n",
      "Epoch: 0007 Avg. cost = 0.055\n",
      "--- 100.29199981689453 seconds ---\n",
      "Accuracy: 0.928500000834465\n",
      "Epoch: 0008 Avg. cost = 0.049\n",
      "--- 99.72399997711182 seconds ---\n",
      "Accuracy: 0.9659000062942504\n",
      "Epoch: 0009 Avg. cost = 0.047\n",
      "--- 99.83499789237976 seconds ---\n",
      "Accuracy: 0.9761000114679337\n",
      "Epoch: 0010 Avg. cost = 0.044\n",
      "--- 100.21500325202942 seconds ---\n",
      "Accuracy: 0.9799000096321105\n",
      "Epoch: 0011 Avg. cost = 0.040\n",
      "--- 100.239994764328 seconds ---\n",
      "Accuracy: 0.9813000118732452\n",
      "Epoch: 0012 Avg. cost = 0.039\n",
      "--- 100.04599833488464 seconds ---\n",
      "Accuracy: 0.9841000097990036\n",
      "Epoch: 0013 Avg. cost = 0.037\n",
      "--- 99.85600233078003 seconds ---\n",
      "Accuracy: 0.9843000108003617\n",
      "Epoch: 0014 Avg. cost = 0.034\n",
      "--- 99.77499604225159 seconds ---\n",
      "Accuracy: 0.9854000115394592\n",
      "Epoch: 0015 Avg. cost = 0.032\n",
      "--- 100.04699730873108 seconds ---\n",
      "Accuracy: 0.9856000107526779\n",
      "Epoch: 0016 Avg. cost = 0.030\n",
      "--- 99.75400447845459 seconds ---\n",
      "Accuracy: 0.9843000102043152\n",
      "Epoch: 0017 Avg. cost = 0.029\n",
      "--- 99.86000752449036 seconds ---\n",
      "Accuracy: 0.9866000103950501\n",
      "Epoch: 0018 Avg. cost = 0.027\n",
      "--- 99.3729989528656 seconds ---\n",
      "Accuracy: 0.9869000101089478\n",
      "Epoch: 0019 Avg. cost = 0.026\n",
      "--- 100.41999769210815 seconds ---\n",
      "Accuracy: 0.9888000094890594\n",
      "Epoch: 0020 Avg. cost = 0.025\n",
      "--- 100.20049691200256 seconds ---\n",
      "Accuracy: 0.986400009393692\n",
      "Epoch: 0021 Avg. cost = 0.024\n",
      "--- 99.94806241989136 seconds ---\n",
      "Accuracy: 0.986500009894371\n",
      "Epoch: 0022 Avg. cost = 0.023\n",
      "--- 99.94300127029419 seconds ---\n",
      "Accuracy: 0.9884000086784362\n",
      "Epoch: 0023 Avg. cost = 0.022\n",
      "--- 99.96699905395508 seconds ---\n",
      "Accuracy: 0.9890000087022781\n",
      "Epoch: 0024 Avg. cost = 0.020\n",
      "--- 100.3790009021759 seconds ---\n",
      "Accuracy: 0.9877000105381012\n",
      "Epoch: 0025 Avg. cost = 0.020\n",
      "--- 100.57426166534424 seconds ---\n",
      "Accuracy: 0.9844000101089477\n",
      "Epoch: 0026 Avg. cost = 0.019\n",
      "--- 100.18399572372437 seconds ---\n",
      "Accuracy: 0.9898000097274781\n",
      "Epoch: 0027 Avg. cost = 0.018\n",
      "--- 99.73300194740295 seconds ---\n",
      "Accuracy: 0.9889000087976456\n",
      "Epoch: 0028 Avg. cost = 0.017\n",
      "--- 100.25399994850159 seconds ---\n",
      "Accuracy: 0.9890000092983245\n",
      "Epoch: 0029 Avg. cost = 0.017\n",
      "--- 99.79200053215027 seconds ---\n",
      "Accuracy: 0.9890000092983245\n",
      "Epoch: 0030 Avg. cost = 0.016\n",
      "--- 99.85700178146362 seconds ---\n",
      "Accuracy: 0.9909000080823899\n",
      "Epoch: 0031 Avg. cost = 0.016\n",
      "--- 99.66399765014648 seconds ---\n",
      "Accuracy: 0.9888000094890594\n",
      "Epoch: 0032 Avg. cost = 0.015\n",
      "--- 99.89299726486206 seconds ---\n",
      "Accuracy: 0.9889000087976456\n",
      "Epoch: 0033 Avg. cost = 0.015\n",
      "--- 99.667001247406 seconds ---\n",
      "Accuracy: 0.98960000872612\n",
      "Epoch: 0034 Avg. cost = 0.014\n",
      "--- 100.03500604629517 seconds ---\n",
      "Accuracy: 0.988900009393692\n",
      "Epoch: 0035 Avg. cost = 0.014\n",
      "--- 100.0229983329773 seconds ---\n",
      "Accuracy: 0.9884000092744827\n",
      "Epoch: 0036 Avg. cost = 0.013\n",
      "--- 99.62199783325195 seconds ---\n",
      "Accuracy: 0.9892000085115433\n",
      "Epoch: 0037 Avg. cost = 0.013\n",
      "--- 99.86799669265747 seconds ---\n",
      "Accuracy: 0.9906000083684922\n",
      "Epoch: 0038 Avg. cost = 0.012\n",
      "--- 99.9280002117157 seconds ---\n",
      "Accuracy: 0.9877000105381012\n",
      "Epoch: 0039 Avg. cost = 0.012\n",
      "--- 99.47300100326538 seconds ---\n",
      "Accuracy: 0.9895000100135803\n",
      "Epoch: 0040 Avg. cost = 0.012\n",
      "--- 99.81199932098389 seconds ---\n",
      "Accuracy: 0.9892000097036362\n",
      "Epoch: 0041 Avg. cost = 0.011\n",
      "--- 99.70799994468689 seconds ---\n",
      "Accuracy: 0.9895000082254409\n",
      "Epoch: 0042 Avg. cost = 0.011\n",
      "--- 99.63400077819824 seconds ---\n",
      "Accuracy: 0.9898000097274781\n",
      "Epoch: 0043 Avg. cost = 0.011\n",
      "--- 99.70755362510681 seconds ---\n",
      "Accuracy: 0.9910000085830688\n",
      "Epoch: 0044 Avg. cost = 0.010\n",
      "--- 100.13600063323975 seconds ---\n",
      "Accuracy: 0.990300008058548\n",
      "Epoch: 0045 Avg. cost = 0.010\n",
      "--- 99.87200021743774 seconds ---\n",
      "Accuracy: 0.9904000091552735\n",
      "Epoch: 0046 Avg. cost = 0.010\n",
      "--- 99.96100306510925 seconds ---\n",
      "Accuracy: 0.9903000092506409\n",
      "Epoch: 0047 Avg. cost = 0.009\n",
      "--- 99.4788339138031 seconds ---\n",
      "Accuracy: 0.9903000086545944\n",
      "Epoch: 0048 Avg. cost = 0.009\n",
      "--- 99.50070810317993 seconds ---\n",
      "Accuracy: 0.9909000086784363\n",
      "Epoch: 0049 Avg. cost = 0.010\n",
      "--- 99.04195499420166 seconds ---\n",
      "Accuracy: 0.989700009226799\n",
      "Epoch: 0050 Avg. cost = 0.009\n",
      "--- 99.8534619808197 seconds ---\n",
      "Accuracy: 0.9887000101804734\n",
      "Epoch: 0051 Avg. cost = 0.009\n",
      "--- 99.68149161338806 seconds ---\n",
      "Accuracy: 0.9898000079393386\n",
      "Epoch: 0052 Avg. cost = 0.008\n",
      "--- 100.11302471160889 seconds ---\n",
      "Accuracy: 0.9887000095844268\n",
      "Epoch: 0053 Avg. cost = 0.008\n",
      "--- 100.06799674034119 seconds ---\n",
      "Accuracy: 0.9893000084161758\n",
      "Epoch: 0054 Avg. cost = 0.008\n",
      "--- 99.89300036430359 seconds ---\n",
      "Accuracy: 0.9895000088214875\n",
      "Epoch: 0055 Avg. cost = 0.008\n",
      "--- 99.93400382995605 seconds ---\n",
      "Accuracy: 0.9907000088691711\n",
      "Epoch: 0056 Avg. cost = 0.007\n",
      "--- 100.18500328063965 seconds ---\n",
      "Accuracy: 0.9898000085353851\n",
      "Epoch: 0057 Avg. cost = 0.008\n",
      "--- 100.19700121879578 seconds ---\n",
      "Accuracy: 0.9902000081539154\n",
      "Epoch: 0058 Avg. cost = 0.007\n",
      "--- 99.95800423622131 seconds ---\n",
      "Accuracy: 0.9896000081300735\n",
      "Epoch: 0059 Avg. cost = 0.007\n",
      "--- 99.74800419807434 seconds ---\n",
      "Accuracy: 0.9894000101089477\n",
      "Epoch: 0060 Avg. cost = 0.007\n",
      "--- 99.67700147628784 seconds ---\n",
      "Accuracy: 0.9898000091314316\n",
      "Epoch: 0061 Avg. cost = 0.007\n",
      "--- 99.80700039863586 seconds ---\n",
      "Accuracy: 0.9904000085592269\n",
      "Epoch: 0062 Avg. cost = 0.006\n",
      "--- 100.38000082969666 seconds ---\n",
      "Accuracy: 0.99180000603199\n",
      "Epoch: 0063 Avg. cost = 0.006\n",
      "--- 100.21600008010864 seconds ---\n",
      "Accuracy: 0.98960000872612\n",
      "Epoch: 0064 Avg. cost = 0.006\n",
      "--- 99.93000769615173 seconds ---\n",
      "Accuracy: 0.9917000073194504\n",
      "Epoch: 0065 Avg. cost = 0.006\n",
      "--- 99.88100576400757 seconds ---\n",
      "Accuracy: 0.9907000076770782\n",
      "Epoch: 0066 Avg. cost = 0.006\n",
      "--- 100.20199537277222 seconds ---\n",
      "Accuracy: 0.9900000083446503\n",
      "Epoch: 0067 Avg. cost = 0.006\n",
      "--- 99.92200565338135 seconds ---\n",
      "Accuracy: 0.9901000076532364\n",
      "Epoch: 0068 Avg. cost = 0.005\n",
      "--- 99.45199656486511 seconds ---\n",
      "Accuracy: 0.9915000069141388\n",
      "Epoch: 0069 Avg. cost = 0.005\n",
      "--- 99.78700017929077 seconds ---\n",
      "Accuracy: 0.990100005865097\n",
      "Epoch: 0070 Avg. cost = 0.005\n",
      "--- 100.02199840545654 seconds ---\n",
      "Accuracy: 0.9901000070571899\n",
      "Epoch: 0071 Avg. cost = 0.005\n",
      "--- 100.40899968147278 seconds ---\n",
      "Accuracy: 0.98960000872612\n",
      "Epoch: 0072 Avg. cost = 0.005\n",
      "--- 100.12900114059448 seconds ---\n",
      "Accuracy: 0.9905000072717667\n",
      "Epoch: 0073 Avg. cost = 0.005\n",
      "--- 99.7620017528534 seconds ---\n",
      "Accuracy: 0.9911000084877014\n",
      "Epoch: 0074 Avg. cost = 0.004\n",
      "--- 100.6309974193573 seconds ---\n",
      "Accuracy: 0.989000009894371\n",
      "Epoch: 0075 Avg. cost = 0.004\n",
      "--- 100.12699866294861 seconds ---\n",
      "Accuracy: 0.9919000071287155\n",
      "Epoch: 0076 Avg. cost = 0.005\n",
      "--- 99.83100414276123 seconds ---\n",
      "Accuracy: 0.9917000079154968\n",
      "Epoch: 0077 Avg. cost = 0.004\n",
      "--- 100.39400219917297 seconds ---\n",
      "Accuracy: 0.9910000067949295\n",
      "Epoch: 0078 Avg. cost = 0.004\n",
      "--- 100.30399560928345 seconds ---\n",
      "Accuracy: 0.9905000078678131\n",
      "Epoch: 0079 Avg. cost = 0.004\n",
      "--- 100.14541840553284 seconds ---\n",
      "Accuracy: 0.9905000084638595\n",
      "Epoch: 0080 Avg. cost = 0.004\n",
      "--- 99.76002287864685 seconds ---\n",
      "Accuracy: 0.9902000081539154\n",
      "Epoch: 0081 Avg. cost = 0.004\n",
      "--- 100.19685292243958 seconds ---\n",
      "Accuracy: 0.991900007724762\n",
      "Epoch: 0082 Avg. cost = 0.004\n",
      "--- 99.65714454650879 seconds ---\n",
      "Accuracy: 0.989700009226799\n",
      "Epoch: 0083 Avg. cost = 0.004\n",
      "--- 99.75697541236877 seconds ---\n",
      "Accuracy: 0.9923000067472458\n",
      "Epoch: 0084 Avg. cost = 0.004\n",
      "--- 99.8703670501709 seconds ---\n",
      "Accuracy: 0.9904000073671341\n",
      "Epoch: 0085 Avg. cost = 0.003\n",
      "--- 99.81064510345459 seconds ---\n",
      "Accuracy: 0.9909000068902969\n",
      "Epoch: 0086 Avg. cost = 0.004\n",
      "--- 99.99819302558899 seconds ---\n",
      "Accuracy: 0.9921000063419342\n",
      "Epoch: 0087 Avg. cost = 0.004\n",
      "--- 100.1874988079071 seconds ---\n",
      "Accuracy: 0.9910000073909759\n",
      "Epoch: 0088 Avg. cost = 0.004\n",
      "--- 100.15625238418579 seconds ---\n",
      "Accuracy: 0.9909000068902969\n",
      "Epoch: 0089 Avg. cost = 0.004\n",
      "--- 99.64062094688416 seconds ---\n",
      "Accuracy: 0.9910000079870224\n",
      "Epoch: 0090 Avg. cost = 0.004\n",
      "--- 99.6874988079071 seconds ---\n",
      "Accuracy: 0.9911000072956085\n",
      "Epoch: 0091 Avg. cost = 0.003\n",
      "--- 100.03124594688416 seconds ---\n",
      "Accuracy: 0.9917000073194504\n",
      "Epoch: 0092 Avg. cost = 0.003\n",
      "--- 100.09375286102295 seconds ---\n",
      "Accuracy: 0.991200008392334\n",
      "Epoch: 0093 Avg. cost = 0.003\n",
      "--- 99.62499737739563 seconds ---\n",
      "Accuracy: 0.9918000078201294\n",
      "Epoch: 0094 Avg. cost = 0.003\n",
      "--- 100.10937595367432 seconds ---\n",
      "Accuracy: 0.9910000079870224\n",
      "Epoch: 0095 Avg. cost = 0.003\n",
      "--- 99.92000246047974 seconds ---\n",
      "Accuracy: 0.9908000069856644\n",
      "Epoch: 0096 Avg. cost = 0.003\n",
      "--- 100.23437452316284 seconds ---\n",
      "Accuracy: 0.9918000072240829\n",
      "Epoch: 0097 Avg. cost = 0.003\n",
      "--- 99.93749904632568 seconds ---\n",
      "Accuracy: 0.9909000086784363\n",
      "Epoch: 0098 Avg. cost = 0.003\n",
      "--- 99.60937094688416 seconds ---\n",
      "Accuracy: 0.9908000075817108\n",
      "Epoch: 0099 Avg. cost = 0.003\n",
      "--- 100.015629529953 seconds ---\n",
      "Accuracy: 0.9916000074148178\n",
      "Epoch: 0100 Avg. cost = 0.003\n",
      "--- 100.40625 seconds ---\n",
      "Accuracy: 0.9902000087499618\n",
      "End of optimization!\n"
     ]
    }
   ],
   "source": [
    "# Loss 함수\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_))\n",
    "\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(update_ops):\n",
    "     train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cost, global_step=global_step)\n",
    "        \n",
    "# variable initializer\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "#sess = tf.Session()\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)\n",
    "\n",
    "batch_size_train = 100\n",
    "batch_size_test = 100   \n",
    "total_train_batch = int(mnist.train.num_examples / batch_size_train)\n",
    "total_test_batch = int(mnist.test.num_examples / batch_size_test)\n",
    "\n",
    "for epoch in range(100):\n",
    "    total_cost = 0\n",
    "    total_accuracy = 0\n",
    "\n",
    "    start_time = time.time()\n",
    "    for i in range(total_train_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size_train)\n",
    "        batch_xs = batch_xs.reshape(-1, 28, 28, 1)\n",
    "        \n",
    "        _, cost_val = sess.run([train_step, cost],\n",
    "                               feed_dict={x: batch_xs,y_: batch_ys, phase: True})\n",
    "        total_cost += cost_val\n",
    "    print('Epoch:', '%04d' % (epoch + 1),\n",
    "          'Avg. cost =', '{:.3f}'.format(total_cost / total_train_batch))\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "        \n",
    "    for i in range(total_test_batch):\n",
    "        batch_xs2, batch_ys2 = mnist.test.next_batch(batch_size_test)   \n",
    "        batch_xs2 = batch_xs2.reshape(-1, 28, 28, 1) \n",
    "        acc = sess.run(accuracy, feed_dict={x: batch_xs2, y_: batch_ys2, phase: False})\n",
    "        total_accuracy += acc    \n",
    "    \n",
    "    mean_accuracy = total_accuracy/total_test_batch\n",
    "    print('Accuracy:', mean_accuracy) \n",
    "          \n",
    "print('End of optimization!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
