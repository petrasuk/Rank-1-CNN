{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#The following sites have been referred to for the implementation of this code.\n",
    "\n",
    "https://github.com/pat-coady/tiny_imagenet\n",
    "\n",
    "http://solarisailab.com/archives/2325"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rank-1 CNN on CIFAR10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import re\n",
    "import os\n",
    "\n",
    "# use matplotlib as inline\n",
    "%matplotlib inline \n",
    "from functools import partial\n",
    "from matplotlib.pyplot import *\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.keras._impl.keras.datasets.cifar10 import load_data\n",
    "\n",
    "# for reading the next batch\n",
    "def next_batch(num, data, labels): \n",
    "    ''' \n",
    "    Return a total of `num` random samples and labels.  \n",
    "    ''' \n",
    "    idx = np.arange(0 , len(data)) \n",
    "    np.random.shuffle(idx) \n",
    "    idx = idx[:num] \n",
    "    data_shuffle = [data[i] for i in idx] \n",
    "    labels_shuffle = [labels[i] for i in idx] \n",
    "\n",
    " \n",
    "    return np.asarray(data_shuffle), np.asarray(labels_shuffle) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " #configuration for GPU to use 90% of its capacity\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allocator_type ='BFC'\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(\"float\", [None, 32, 32, 3])\n",
    "\n",
    "phase = tf.placeholder(tf.bool, name='phase') #whether batch normalization is in its training or test mode\n",
    "\n",
    "# conv1_1\n",
    "input_num = 3  #number of input channels\n",
    "dim = 3 # Size of the 1-D vectors\n",
    "num = 8 # Number of P and Q vectors. The combination of P and Q vectors becomes num * num\n",
    "output_num = num*num   # number of filters = number of outputs\n",
    "\n",
    "W_list1_1 = [] # list for W filter\n",
    "\n",
    "P1_1 = tf.get_variable(\"P1_1\", shape=[num,dim],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "Q1_1 = tf.get_variable(\"Q1_1\", shape=[num,dim],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "T1_1 = tf.get_variable(\"T1_1\", shape=[output_num,input_num,1,1],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "k=0\n",
    "for i in range(num): \n",
    "    for j in range(num):\n",
    "      P1_1_temp = tf.reshape(P1_1[i], [dim, 1])\n",
    "      Q1_1_temp = tf.reshape(Q1_1[j], [1, dim])\n",
    "      Mul_temp1_1_a = tf.matmul(P1_1_temp, Q1_1_temp)\n",
    "      Mul_temp1_1_b = tf.reshape(Mul_temp1_1_a, [1, Mul_temp1_1_a.shape[0], Mul_temp1_1_a.shape[1]])\n",
    "      Mul_temp1_1_c = T1_1[k]*Mul_temp1_1_b\n",
    "      k = k + 1\n",
    "      W_list1_1.append(Mul_temp1_1_c)\n",
    "   \n",
    "    \n",
    "W1_1 = tf.transpose(tf.stack(W_list1_1, axis=0))\n",
    "\n",
    "L1 = tf.nn.conv2d(x, W1_1, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "Bias1_1 = tf.Variable(tf.constant(0.0, shape=[output_num], dtype=tf.float32),\n",
    "                                 trainable=True, name='Bias1_1')\n",
    "L1 = tf.nn.bias_add(L1, Bias1_1)\n",
    "L1 = tf.nn.relu(L1)\n",
    "# batch normalization\n",
    "L1 = tf.layers.batch_normalization(L1, training=phase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# conv1_2\n",
    "input_num = 64  #number of input channels\n",
    "dim = 3 # Size of the 1-D vectors\n",
    "num = 8 # Number of P and Q vectors. The combination of P and Q vectors becomes num * num\n",
    "output_num = num*num # number of filters = number of outputs\n",
    "\n",
    "W_list1_2 = [] # list for W filter\n",
    "#W = tf.Variable(tf.random_uniform([10,3,3], -0.5, 0.5))\n",
    "\n",
    "P1_2 = tf.get_variable(\"P1_2\", shape=[num,dim],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "Q1_2 = tf.get_variable(\"Q1_2\", shape=[num,dim],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "T1_2 = tf.get_variable(\"T1_2\", shape=[output_num,input_num,1,1],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "k=0\n",
    "for i in range(num): \n",
    "    for j in range(num):\n",
    "      P1_2_temp = tf.reshape(P1_2[i], [dim, 1])\n",
    "      Q1_2_temp = tf.reshape(Q1_2[j], [1, dim])\n",
    "      Mul_temp1_2_a = tf.matmul(P1_2_temp, Q1_2_temp)\n",
    "      Mul_temp1_2_b = tf.reshape(Mul_temp1_2_a, [1, Mul_temp1_2_a.shape[0], Mul_temp1_2_a.shape[1]])\n",
    "      Mul_temp1_2_c = T1_2[k]*Mul_temp1_2_b\n",
    "      k = k + 1\n",
    "      W_list1_2.append(Mul_temp1_2_c)\n",
    "\n",
    "W1_2 = tf.transpose(tf.stack(W_list1_2, axis=0))\n",
    "\n",
    "L1 = tf.nn.conv2d(L1, W1_2, strides=[1, 1, 1, 1], padding='SAME')\n",
    "Bias1_2 = tf.Variable(tf.constant(0.0, shape=[output_num], dtype=tf.float32),\n",
    "                                 trainable=True, name='Bias1_2')\n",
    "L1 = tf.nn.bias_add(L1, Bias1_2)\n",
    "# batch normalization\n",
    "#L1 = tf.layers.batch_normalization(L1, training=phase)\n",
    "\n",
    "L1 = tf.nn.relu(L1)\n",
    "# Max_Pool1\n",
    "# 입력 : (N, 160, 160, 64)  --> 출력 : (N, 80, 80, 64)\n",
    "L1 = tf.nn.max_pool(L1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "L1 = tf.nn.dropout(L1, 0.5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# conv2_1\n",
    "\n",
    "input_num = 64    #number of input channels\n",
    "dim = 3 # Size of the 1-D vectors\n",
    "num = 12 # Number of P and Q vectors. The combination of P and Q vectors becomes num * num\n",
    "output_num = num*num  # number of filters = number of outputs\n",
    "\n",
    "W_list2_1 = [] # list for W filter\n",
    "\n",
    "P2_1 = tf.get_variable(\"P2_1\", shape=[num,dim],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "Q2_1 = tf.get_variable(\"Q2_1\", shape=[num,dim],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "T2_1 = tf.get_variable(\"T2_1\", shape=[output_num,input_num,1,1],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "k=0\n",
    "for i in range(num): \n",
    "    for j in range(num):\n",
    "      P2_1_temp = tf.reshape(P2_1[i], [dim, 1])\n",
    "      Q2_1_temp = tf.reshape(Q2_1[j], [1, dim])\n",
    "      Mul_temp2_1_a = tf.matmul(P2_1_temp, Q2_1_temp)\n",
    "      Mul_temp2_1_b = tf.reshape(Mul_temp2_1_a, [1, Mul_temp2_1_a.shape[0], Mul_temp2_1_a.shape[1]])\n",
    "      Mul_temp2_1_c = T2_1[k]*Mul_temp2_1_b\n",
    "      k = k + 1\n",
    "      W_list2_1.append(Mul_temp2_1_c)\n",
    "\n",
    "W2_1 = tf.transpose(tf.stack(W_list2_1, axis=0))\n",
    "\n",
    "L2 = tf.nn.conv2d(L1, W2_1, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "Bias2_1 = tf.Variable(tf.constant(0.0, shape=[output_num], dtype=tf.float32),\n",
    "                                 trainable=True, name='Bias2_1')\n",
    "L2 = tf.nn.bias_add(L2, Bias2_1)\n",
    "\n",
    "L2 = tf.nn.relu(L2)\n",
    "\n",
    "# batch normalization\n",
    "L2 = tf.layers.batch_normalization(L2, training=phase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# conv2_2\n",
    "\n",
    "input_num = 144    #number of input channels\n",
    "dim = 3 # Size of the 1-D vectors\n",
    "num = 12 # Number of P and Q vectors. The combination of P and Q vectors becomes num * num\n",
    "output_num = num*num  # number of filters = number of outputs\n",
    "\n",
    "W_list2_2 = [] # list for W filter\n",
    "\n",
    "P2_2 = tf.get_variable(\"P2_2\", shape=[num,dim],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "Q2_2 = tf.get_variable(\"Q2_2\", shape=[num,dim],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "T2_2 = tf.get_variable(\"T2_2\", shape=[output_num,input_num,1,1],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "k=0\n",
    "for i in range(num): \n",
    "    for j in range(num):\n",
    "      P2_2_temp = tf.reshape(P2_2[i], [dim, 1])\n",
    "      Q2_2_temp = tf.reshape(Q2_2[j], [1, dim])\n",
    "      Mul_temp2_2_a = tf.matmul(P2_2_temp, Q2_2_temp)\n",
    "      Mul_temp2_2_b = tf.reshape(Mul_temp2_2_a, [1, Mul_temp2_2_a.shape[0], Mul_temp2_2_a.shape[1]])\n",
    "      Mul_temp2_2_c = T2_2[k]*Mul_temp2_2_b\n",
    "      k = k + 1\n",
    "      W_list2_2.append(Mul_temp2_2_c)\n",
    "\n",
    "W2_2 = tf.transpose(tf.stack(W_list2_2, axis=0))\n",
    "\n",
    "L2 = tf.nn.conv2d(L2, W2_2, strides=[1, 1, 1, 1], padding='SAME')\n",
    "Bias2_2 = tf.Variable(tf.constant(0.0, shape=[output_num], dtype=tf.float32),\n",
    "                                 trainable=True, name='Bias2_2')\n",
    "L2 = tf.nn.bias_add(L2, Bias2_2)\n",
    "L2 = tf.nn.relu(L2)\n",
    "L2 = tf.nn.max_pool(L2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "L2 = tf.nn.dropout(L2, 0.5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# conv3_1\n",
    "\n",
    "input_num = 144    #number of input channels\n",
    "dim = 3 # Size of the 1-D vectors\n",
    "num = 16 # Number of P and Q vectors. The combination of P and Q vectors becomes num * num\n",
    "output_num = num*num # number of filters = number of outputs\n",
    "\n",
    "W_list3_1 = [] # list for W filter\n",
    "\n",
    "P3_1 = tf.get_variable(\"P3_1\", shape=[num,dim],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "Q3_1 = tf.get_variable(\"Q3_1\", shape=[num,dim],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "T3_1 = tf.get_variable(\"T3_1\", shape=[output_num,input_num,1,1],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "k=0\n",
    "for i in range(num): \n",
    "    for j in range(num):\n",
    "      P3_1_temp = tf.reshape(P3_1[i], [dim, 1])\n",
    "      Q3_1_temp = tf.reshape(Q3_1[j], [1, dim])\n",
    "      Mul_temp3_1_a = tf.matmul(P3_1_temp, Q3_1_temp)\n",
    "      Mul_temp3_1_b = tf.reshape(Mul_temp3_1_a, [1, Mul_temp3_1_a.shape[0], Mul_temp3_1_a.shape[1]])\n",
    "      Mul_temp3_1_c = T3_1[k]*Mul_temp3_1_b\n",
    "      k = k + 1\n",
    "      W_list3_1.append(Mul_temp3_1_c)\n",
    "\n",
    "W3_1 = tf.transpose(tf.stack(W_list3_1, axis=0))\n",
    "\n",
    "L3 = tf.nn.conv2d(L2, W3_1, strides=[1, 1, 1, 1], padding='SAME')\n",
    "Bias3_1 = tf.Variable(tf.constant(0.0, shape=[output_num], dtype=tf.float32),\n",
    "                                 trainable=True, name='Bias3_1')\n",
    "L3 = tf.nn.bias_add(L3, Bias3_1)\n",
    "\n",
    "L3 = tf.nn.relu(L3)\n",
    "# batch normalization\n",
    "L3 = tf.layers.batch_normalization(L3, training=phase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# conv3_2\n",
    "\n",
    "input_num = 256    #number of input channels\n",
    "dim = 3  # Size of the 1-D vectors \n",
    "num = 16 # Number of P and Q vectors. The combination of P and Q vectors becomes num * num\n",
    "output_num = num*num # number of filters = number of outputs\n",
    "\n",
    "W_list3_2 = [] # list for W filter\n",
    "\n",
    "P3_2 = tf.get_variable(\"P3_2\", shape=[num,dim],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "Q3_2 = tf.get_variable(\"Q3_2\", shape=[num,dim],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "T3_2 = tf.get_variable(\"T3_2\", shape=[output_num,input_num,1,1],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "k=0\n",
    "for i in range(num): \n",
    "    for j in range(num):\n",
    "      P3_2_temp = tf.reshape(P3_2[i], [dim, 1])\n",
    "      Q3_2_temp = tf.reshape(Q3_2[j], [1, dim])\n",
    "      Mul_temp3_2_a = tf.matmul(P3_2_temp, Q3_2_temp)\n",
    "      Mul_temp3_2_b = tf.reshape(Mul_temp3_2_a, [1, Mul_temp3_2_a.shape[0], Mul_temp3_2_a.shape[1]])\n",
    "      Mul_temp3_2_c = T3_2[k]*Mul_temp3_2_b\n",
    "      k = k + 1\n",
    "      W_list3_2.append(Mul_temp3_2_c)\n",
    "\n",
    "W3_2 = tf.transpose(tf.stack(W_list3_2, axis=0))\n",
    "\n",
    "L3 = tf.nn.conv2d(L3, W3_2, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "Bias3_2 = tf.Variable(tf.constant(0.0, shape=[output_num], dtype=tf.float32),\n",
    "                                 trainable=True, name='Bias3_2')\n",
    "L3 = tf.nn.bias_add(L3, Bias3_2)\n",
    "L3 = tf.nn.relu(L3)\n",
    "L3 = tf.nn.max_pool(L3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "L3 = tf.nn.dropout(L3, 0.5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dense(inputs, units, name=None): \n",
    "#\"\"\"3x3 conv layer: ReLU + He initialization\"\"\" \n",
    "# He initialization: normal dist with stdev = sqrt(2.0/fan-in) \n",
    "    stddev = np.sqrt(2 / int(inputs.shape[1])) \n",
    "    out = tf.layers.dense(inputs, units, \n",
    "                           kernel_initializer=tf.contrib.layers.xavier_initializer(), \n",
    "                           kernel_regularizer=tf.contrib.layers.l2_regularizer(1.0), \n",
    "                           name=name) \n",
    "\n",
    "    return out \n",
    "\n",
    "def dense_relu(inputs, units, name=None): \n",
    "    # \"\"\"3x3 conv layer: ReLU + He initialization\"\"\"   \n",
    "    # He initialization: normal dist with stdev = sqrt(2.0/fanin) \n",
    "    stddev = np.sqrt(2 / int(inputs.shape[1])) \n",
    "    out = tf.layers.dense(inputs, units, activation=tf.nn.relu, \n",
    "                         kernel_initializer=tf.contrib.layers.xavier_initializer(), \n",
    "                         kernel_regularizer=tf.contrib.layers.l2_regularizer(1.0), \n",
    "                         name=name) \n",
    "  \n",
    "    return out \n",
    "\n",
    "def dense_batch_relu(x, output, phase, scope):\n",
    "    with tf.variable_scope(scope):\n",
    "        h1 = tf.contrib.layers.fully_connected(x, output, \n",
    "                                               activation_fn=None,\n",
    "                                               scope='dense')\n",
    "        h2 = tf.contrib.layers.batch_norm(h1, \n",
    "                                          center=True, scale=True, \n",
    "                                          is_training=phase,\n",
    "                                          scope='bn')\n",
    "        return tf.nn.relu(h2, 'relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_ = tf.placeholder(\"float\", [None,10])\n",
    "\n",
    "out = tf.contrib.layers.flatten(L3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out = dense_batch_relu(out, 1024, phase,'fc1')\n",
    "out = tf.nn.dropout(out, 0.5) \n",
    "out = dense_batch_relu(out, 512, phase,'fc2')\n",
    "out = tf.nn.dropout(out, 0.5) \n",
    "\n",
    "class_number = 10\n",
    "logits = dense(out, class_number, 'fc3')  # class number =2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Downloading the CIFAR-10 dataset and read them in.\n",
    "def normalize(X_train,X_test): \n",
    "        #this function normalize inputs for zero mean and unit variance \n",
    "        # it is used when training a model. \n",
    "        # Input: training set and test set \n",
    "        # Output: normalized training set and test set according to the trianing set statistics. \n",
    "        X_train = X_train-128\n",
    "        X_test =  X_test-128\n",
    "#        mean = np.mean(X_train,axis=(0,1,2,3)) \n",
    "#        std = np.std(X_train, axis=(0, 1, 2, 3)) \n",
    "#        X_train = (X_train-mean)/(std+1e-7) \n",
    "#        X_test = (X_test-mean)/(std+1e-7) \n",
    "        return X_train, X_test \n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = load_data() \n",
    "x_train = x_train.astype('float32') \n",
    "x_test = x_test.astype('float32') \n",
    "x_train, x_test = normalize(x_train, x_test) \n",
    "\n",
    "# Labels into One-hot Encoding form\n",
    "y_train_one_hot = tf.squeeze(tf.one_hot(y_train, 10),axis=1) \n",
    "y_test_one_hot = tf.squeeze(tf.one_hot(y_test, 10),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#########\n",
    "# Result\n",
    "########\n",
    "is_correct = tf.equal(tf.argmax(logits, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-21-cb63ea9c01c7>:2: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "Epoch: 0001 Avg. cost = 856.784\n",
      "--- 94.62800216674805 seconds ---\n",
      "Accuracy: 0.2781000010669231\n",
      "Epoch: 0002 Avg. cost = 555.322\n",
      "--- 80.51699876785278 seconds ---\n",
      "Accuracy: 0.49449999779462817\n",
      "Epoch: 0003 Avg. cost = 474.815\n",
      "--- 80.92648458480835 seconds ---\n",
      "Accuracy: 0.6300000029802323\n",
      "Epoch: 0004 Avg. cost = 427.712\n",
      "--- 80.70384764671326 seconds ---\n",
      "Accuracy: 0.6370999997854233\n",
      "Epoch: 0005 Avg. cost = 399.844\n",
      "--- 80.90805196762085 seconds ---\n",
      "Accuracy: 0.6721999990940094\n",
      "Epoch: 0006 Avg. cost = 377.702\n",
      "--- 80.89074444770813 seconds ---\n",
      "Accuracy: 0.7115000027418137\n",
      "Epoch: 0007 Avg. cost = 361.640\n",
      "--- 80.9800271987915 seconds ---\n",
      "Accuracy: 0.7068000024557114\n",
      "Epoch: 0008 Avg. cost = 337.694\n",
      "--- 80.63636898994446 seconds ---\n",
      "Accuracy: 0.7175\n",
      "Epoch: 0009 Avg. cost = 328.702\n",
      "--- 80.5608639717102 seconds ---\n",
      "Accuracy: 0.7313000005483627\n",
      "Epoch: 0010 Avg. cost = 317.121\n",
      "--- 81.05394506454468 seconds ---\n",
      "Accuracy: 0.736100002527237\n",
      "Epoch: 0011 Avg. cost = 303.860\n",
      "--- 80.94547510147095 seconds ---\n",
      "Accuracy: 0.7361000019311905\n",
      "Epoch: 0012 Avg. cost = 290.831\n",
      "--- 80.71594524383545 seconds ---\n",
      "Accuracy: 0.7383000016212463\n",
      "Epoch: 0013 Avg. cost = 284.616\n",
      "--- 80.8792028427124 seconds ---\n",
      "Accuracy: 0.7462999975681305\n",
      "Epoch: 0014 Avg. cost = 276.126\n",
      "--- 80.97793221473694 seconds ---\n",
      "Accuracy: 0.7714000016450882\n",
      "Epoch: 0015 Avg. cost = 267.471\n",
      "--- 80.94796395301819 seconds ---\n",
      "Accuracy: 0.7795999985933304\n",
      "Epoch: 0016 Avg. cost = 259.893\n",
      "--- 80.74940514564514 seconds ---\n",
      "Accuracy: 0.7698999977111817\n",
      "Epoch: 0017 Avg. cost = 254.499\n",
      "--- 81.088632106781 seconds ---\n",
      "Accuracy: 0.7846999984979629\n",
      "Epoch: 0018 Avg. cost = 243.093\n",
      "--- 81.18591642379761 seconds ---\n",
      "Accuracy: 0.781999996304512\n",
      "Epoch: 0019 Avg. cost = 237.888\n",
      "--- 81.1806571483612 seconds ---\n",
      "Accuracy: 0.7890999972820282\n",
      "Epoch: 0020 Avg. cost = 231.590\n",
      "--- 80.816730260849 seconds ---\n",
      "Accuracy: 0.7843999987840653\n",
      "Epoch: 0021 Avg. cost = 225.302\n",
      "--- 80.98082041740417 seconds ---\n",
      "Accuracy: 0.7817999976873398\n",
      "Epoch: 0022 Avg. cost = 222.020\n",
      "--- 80.55703568458557 seconds ---\n",
      "Accuracy: 0.781700000166893\n",
      "Epoch: 0023 Avg. cost = 217.074\n",
      "--- 81.22202062606812 seconds ---\n",
      "Accuracy: 0.7951999968290329\n",
      "Epoch: 0024 Avg. cost = 213.072\n",
      "--- 81.2223436832428 seconds ---\n",
      "Accuracy: 0.7936999988555908\n",
      "Epoch: 0025 Avg. cost = 207.704\n",
      "--- 80.85957646369934 seconds ---\n",
      "Accuracy: 0.7959999984502792\n",
      "Epoch: 0026 Avg. cost = 203.675\n",
      "--- 80.98383665084839 seconds ---\n",
      "Accuracy: 0.7985999989509582\n",
      "Epoch: 0027 Avg. cost = 199.723\n",
      "--- 80.75517773628235 seconds ---\n",
      "Accuracy: 0.7995999950170517\n",
      "Epoch: 0028 Avg. cost = 194.205\n",
      "--- 80.52029395103455 seconds ---\n",
      "Accuracy: 0.7940999978780746\n",
      "Epoch: 0029 Avg. cost = 192.274\n",
      "--- 81.04338502883911 seconds ---\n",
      "Accuracy: 0.8033999985456467\n",
      "Epoch: 0030 Avg. cost = 190.013\n",
      "--- 80.6027410030365 seconds ---\n",
      "Accuracy: 0.802299998998642\n",
      "Epoch: 0031 Avg. cost = 185.373\n",
      "--- 81.14164209365845 seconds ---\n",
      "Accuracy: 0.7959999966621399\n",
      "Epoch: 0032 Avg. cost = 183.181\n",
      "--- 81.0483193397522 seconds ---\n",
      "Accuracy: 0.8042999994754791\n",
      "Epoch: 0033 Avg. cost = 179.864\n",
      "--- 80.98174476623535 seconds ---\n",
      "Accuracy: 0.8056000006198883\n",
      "Epoch: 0034 Avg. cost = 174.238\n",
      "--- 80.90467023849487 seconds ---\n",
      "Accuracy: 0.8043000000715256\n",
      "Epoch: 0035 Avg. cost = 176.245\n",
      "--- 81.15649437904358 seconds ---\n",
      "Accuracy: 0.8056999957561493\n",
      "Epoch: 0036 Avg. cost = 169.513\n",
      "--- 81.3074541091919 seconds ---\n",
      "Accuracy: 0.8075999963283539\n",
      "Epoch: 0037 Avg. cost = 170.183\n",
      "--- 80.50315403938293 seconds ---\n",
      "Accuracy: 0.8017999947071075\n",
      "Epoch: 0038 Avg. cost = 164.882\n",
      "--- 80.77511286735535 seconds ---\n",
      "Accuracy: 0.8107999992370606\n",
      "Epoch: 0039 Avg. cost = 162.237\n",
      "--- 81.09638547897339 seconds ---\n",
      "Accuracy: 0.8009999990463257\n",
      "Epoch: 0040 Avg. cost = 159.122\n",
      "--- 81.06301975250244 seconds ---\n",
      "Accuracy: 0.8096999961137772\n",
      "Epoch: 0041 Avg. cost = 156.882\n",
      "--- 80.87201356887817 seconds ---\n",
      "Accuracy: 0.7996999961137772\n",
      "Epoch: 0042 Avg. cost = 153.506\n",
      "--- 80.80197143554688 seconds ---\n",
      "Accuracy: 0.8058999991416931\n",
      "Epoch: 0043 Avg. cost = 155.331\n",
      "--- 81.07274866104126 seconds ---\n",
      "Accuracy: 0.812799996137619\n",
      "Epoch: 0044 Avg. cost = 153.039\n",
      "--- 80.88278198242188 seconds ---\n",
      "Accuracy: 0.8111999970674515\n",
      "Epoch: 0045 Avg. cost = 150.202\n",
      "--- 80.81132698059082 seconds ---\n",
      "Accuracy: 0.8071999967098236\n",
      "Epoch: 0046 Avg. cost = 149.781\n",
      "--- 80.87450194358826 seconds ---\n",
      "Accuracy: 0.8037999981641769\n",
      "Epoch: 0047 Avg. cost = 144.746\n",
      "--- 81.29473519325256 seconds ---\n",
      "Accuracy: 0.8158999973535538\n",
      "Epoch: 0048 Avg. cost = 140.103\n",
      "--- 80.83678650856018 seconds ---\n",
      "Accuracy: 0.8153999996185303\n",
      "Epoch: 0049 Avg. cost = 138.459\n",
      "--- 81.13871479034424 seconds ---\n",
      "Accuracy: 0.8134999960660935\n",
      "Epoch: 0050 Avg. cost = 139.977\n",
      "--- 81.07480716705322 seconds ---\n",
      "Accuracy: 0.8107999968528747\n",
      "Epoch: 0051 Avg. cost = 136.606\n",
      "--- 80.82270622253418 seconds ---\n",
      "Accuracy: 0.8132999980449677\n",
      "Epoch: 0052 Avg. cost = 130.744\n",
      "--- 81.1341872215271 seconds ---\n",
      "Accuracy: 0.8237000000476837\n",
      "Epoch: 0053 Avg. cost = 131.943\n",
      "--- 81.10108137130737 seconds ---\n",
      "Accuracy: 0.8141000014543534\n",
      "Epoch: 0054 Avg. cost = 134.257\n",
      "--- 80.90149021148682 seconds ---\n",
      "Accuracy: 0.8171000009775162\n",
      "Epoch: 0055 Avg. cost = 130.279\n",
      "--- 81.06206846237183 seconds ---\n",
      "Accuracy: 0.820399997830391\n",
      "Epoch: 0056 Avg. cost = 128.671\n",
      "--- 81.1291275024414 seconds ---\n",
      "Accuracy: 0.8141999959945678\n",
      "Epoch: 0057 Avg. cost = 128.513\n",
      "--- 81.30917835235596 seconds ---\n",
      "Accuracy: 0.8113999992609024\n",
      "Epoch: 0058 Avg. cost = 126.210\n",
      "--- 81.34407138824463 seconds ---\n",
      "Accuracy: 0.8149999976158142\n",
      "Epoch: 0059 Avg. cost = 127.423\n",
      "--- 80.93123722076416 seconds ---\n",
      "Accuracy: 0.8206999987363816\n",
      "Epoch: 0060 Avg. cost = 120.588\n",
      "--- 81.31561017036438 seconds ---\n",
      "Accuracy: 0.813999997973442\n",
      "Epoch: 0061 Avg. cost = 119.767\n",
      "--- 80.6219379901886 seconds ---\n",
      "Accuracy: 0.8235999983549118\n",
      "Epoch: 0062 Avg. cost = 121.202\n",
      "--- 81.00407981872559 seconds ---\n",
      "Accuracy: 0.8129999953508377\n",
      "Epoch: 0063 Avg. cost = 117.286\n",
      "--- 80.49624705314636 seconds ---\n",
      "Accuracy: 0.8176999998092651\n",
      "Epoch: 0064 Avg. cost = 118.705\n",
      "--- 80.70093822479248 seconds ---\n",
      "Accuracy: 0.8106999963521957\n",
      "Epoch: 0065 Avg. cost = 117.682\n",
      "--- 80.81716132164001 seconds ---\n",
      "Accuracy: 0.8256000006198883\n",
      "Epoch: 0066 Avg. cost = 115.279\n",
      "--- 81.49748063087463 seconds ---\n",
      "Accuracy: 0.8201999992132187\n",
      "Epoch: 0067 Avg. cost = 112.089\n",
      "--- 80.80373811721802 seconds ---\n",
      "Accuracy: 0.8136000007390976\n",
      "Epoch: 0068 Avg. cost = 112.688\n",
      "--- 80.65262818336487 seconds ---\n",
      "Accuracy: 0.8160999977588653\n",
      "Epoch: 0069 Avg. cost = 112.182\n",
      "--- 81.38841819763184 seconds ---\n",
      "Accuracy: 0.8210000020265579\n",
      "Epoch: 0070 Avg. cost = 111.769\n",
      "--- 80.99748253822327 seconds ---\n",
      "Accuracy: 0.8164999961853028\n",
      "Epoch: 0071 Avg. cost = 110.992\n",
      "--- 80.82414507865906 seconds ---\n",
      "Accuracy: 0.8212000012397767\n",
      "Epoch: 0072 Avg. cost = 106.510\n",
      "--- 80.67146158218384 seconds ---\n",
      "Accuracy: 0.8201999938488007\n",
      "Epoch: 0073 Avg. cost = 107.767\n",
      "--- 81.15712833404541 seconds ---\n",
      "Accuracy: 0.8203999960422516\n",
      "Epoch: 0074 Avg. cost = 107.788\n",
      "--- 81.41262578964233 seconds ---\n",
      "Accuracy: 0.8282999986410141\n",
      "Epoch: 0075 Avg. cost = 107.551\n",
      "--- 80.87131118774414 seconds ---\n",
      "Accuracy: 0.820099995136261\n",
      "Epoch: 0076 Avg. cost = 107.642\n",
      "--- 80.64768719673157 seconds ---\n",
      "Accuracy: 0.8203999984264374\n",
      "Epoch: 0077 Avg. cost = 101.926\n",
      "--- 81.34143877029419 seconds ---\n",
      "Accuracy: 0.8182000017166138\n",
      "Epoch: 0078 Avg. cost = 100.299\n",
      "--- 80.85990858078003 seconds ---\n",
      "Accuracy: 0.8199999970197678\n",
      "Epoch: 0079 Avg. cost = 100.253\n",
      "--- 80.6736192703247 seconds ---\n",
      "Accuracy: 0.8206999981403351\n",
      "Epoch: 0080 Avg. cost = 100.318\n",
      "--- 80.73275828361511 seconds ---\n",
      "Accuracy: 0.8192000007629394\n",
      "Epoch: 0081 Avg. cost = 101.656\n",
      "--- 80.82487869262695 seconds ---\n",
      "Accuracy: 0.817199998497963\n",
      "Epoch: 0082 Avg. cost = 96.673\n",
      "--- 80.87244153022766 seconds ---\n",
      "Accuracy: 0.8152999985218048\n",
      "Epoch: 0083 Avg. cost = 100.447\n",
      "--- 80.71263098716736 seconds ---\n",
      "Accuracy: 0.8243999981880188\n",
      "Epoch: 0084 Avg. cost = 97.246\n",
      "--- 81.16941094398499 seconds ---\n",
      "Accuracy: 0.8177999973297119\n",
      "Epoch: 0085 Avg. cost = 97.063\n",
      "--- 81.13687324523926 seconds ---\n",
      "Accuracy: 0.8229999959468841\n",
      "Epoch: 0086 Avg. cost = 93.868\n",
      "--- 80.63508248329163 seconds ---\n",
      "Accuracy: 0.8247999984025955\n",
      "Epoch: 0087 Avg. cost = 93.998\n",
      "--- 81.13735556602478 seconds ---\n",
      "Accuracy: 0.8155999988317489\n",
      "Epoch: 0088 Avg. cost = 95.234\n",
      "--- 81.09004330635071 seconds ---\n",
      "Accuracy: 0.8122999966144562\n",
      "Epoch: 0089 Avg. cost = 95.117\n",
      "--- 81.14863276481628 seconds ---\n",
      "Accuracy: 0.8214999961853028\n",
      "Epoch: 0090 Avg. cost = 95.537\n",
      "--- 80.66323280334473 seconds ---\n",
      "Accuracy: 0.816700000166893\n",
      "Epoch: 0091 Avg. cost = 91.985\n",
      "--- 81.12938356399536 seconds ---\n",
      "Accuracy: 0.8166999977827072\n",
      "Epoch: 0092 Avg. cost = 94.587\n",
      "--- 80.77737879753113 seconds ---\n",
      "Accuracy: 0.8261999970674515\n",
      "Epoch: 0093 Avg. cost = 91.324\n",
      "--- 81.21066999435425 seconds ---\n",
      "Accuracy: 0.8215000009536744\n",
      "Epoch: 0094 Avg. cost = 89.228\n",
      "--- 80.91809296607971 seconds ---\n",
      "Accuracy: 0.8235999965667724\n",
      "Epoch: 0095 Avg. cost = 89.408\n",
      "--- 81.10371804237366 seconds ---\n",
      "Accuracy: 0.8236999994516373\n",
      "Epoch: 0096 Avg. cost = 88.635\n",
      "--- 80.97016334533691 seconds ---\n",
      "Accuracy: 0.8195999979972839\n",
      "Epoch: 0097 Avg. cost = 91.163\n",
      "--- 80.80195713043213 seconds ---\n",
      "Accuracy: 0.8177999979257584\n",
      "Epoch: 0098 Avg. cost = 89.430\n",
      "--- 81.10145425796509 seconds ---\n",
      "Accuracy: 0.8259999972581863\n",
      "Epoch: 0099 Avg. cost = 90.086\n",
      "--- 80.99353122711182 seconds ---\n",
      "Accuracy: 0.8177999973297119\n",
      "Epoch: 0100 Avg. cost = 89.143\n",
      "--- 81.1069233417511 seconds ---\n",
      "Accuracy: 0.8228999978303909\n",
      "Epoch: 0101 Avg. cost = 83.354\n",
      "--- 80.7912962436676 seconds ---\n",
      "Accuracy: 0.8208999967575074\n",
      "Epoch: 0102 Avg. cost = 88.153\n",
      "--- 80.69894933700562 seconds ---\n",
      "Accuracy: 0.8183999997377396\n",
      "Epoch: 0103 Avg. cost = 85.841\n",
      "--- 80.80209755897522 seconds ---\n",
      "Accuracy: 0.8226999986171722\n",
      "Epoch: 0104 Avg. cost = 83.301\n",
      "--- 81.53140091896057 seconds ---\n",
      "Accuracy: 0.8171000015735627\n",
      "Epoch: 0105 Avg. cost = 84.575\n",
      "--- 80.7453031539917 seconds ---\n",
      "Accuracy: 0.8222999966144562\n",
      "Epoch: 0106 Avg. cost = 83.089\n",
      "--- 81.42213201522827 seconds ---\n",
      "Accuracy: 0.8257999980449676\n",
      "Epoch: 0107 Avg. cost = 83.007\n",
      "--- 80.64197659492493 seconds ---\n",
      "Accuracy: 0.8231999981403351\n",
      "Epoch: 0108 Avg. cost = 84.363\n",
      "--- 81.07740640640259 seconds ---\n",
      "Accuracy: 0.8203000003099441\n",
      "Epoch: 0109 Avg. cost = 83.882\n",
      "--- 80.80659937858582 seconds ---\n",
      "Accuracy: 0.8188999968767167\n",
      "Epoch: 0110 Avg. cost = 80.446\n",
      "--- 80.2903299331665 seconds ---\n",
      "Accuracy: 0.8229999971389771\n",
      "Epoch: 0111 Avg. cost = 82.592\n",
      "--- 80.92146587371826 seconds ---\n",
      "Accuracy: 0.81769999563694\n",
      "Epoch: 0112 Avg. cost = 80.212\n",
      "--- 80.4324893951416 seconds ---\n",
      "Accuracy: 0.8237999987602234\n",
      "Epoch: 0113 Avg. cost = 80.254\n",
      "--- 80.55844807624817 seconds ---\n",
      "Accuracy: 0.8295999938249587\n",
      "Epoch: 0114 Avg. cost = 81.522\n",
      "--- 81.07657122612 seconds ---\n",
      "Accuracy: 0.8270999968051911\n",
      "Epoch: 0115 Avg. cost = 79.302\n",
      "--- 80.252596616745 seconds ---\n",
      "Accuracy: 0.829899998307228\n",
      "Epoch: 0116 Avg. cost = 79.896\n",
      "--- 81.10422086715698 seconds ---\n",
      "Accuracy: 0.8208999967575074\n",
      "Epoch: 0117 Avg. cost = 78.132\n",
      "--- 81.05717658996582 seconds ---\n",
      "Accuracy: 0.8238999962806701\n",
      "Epoch: 0118 Avg. cost = 77.377\n",
      "--- 80.37602400779724 seconds ---\n",
      "Accuracy: 0.8212999963760376\n",
      "Epoch: 0119 Avg. cost = 81.255\n",
      "--- 80.45505619049072 seconds ---\n",
      "Accuracy: 0.8235999995470047\n",
      "Epoch: 0120 Avg. cost = 77.630\n",
      "--- 81.04230976104736 seconds ---\n",
      "Accuracy: 0.8196999967098236\n",
      "Epoch: 0121 Avg. cost = 76.258\n",
      "--- 80.48260021209717 seconds ---\n",
      "Accuracy: 0.8224999958276749\n",
      "Epoch: 0122 Avg. cost = 77.536\n",
      "--- 81.1223316192627 seconds ---\n",
      "Accuracy: 0.8190999984741211\n",
      "Epoch: 0123 Avg. cost = 76.429\n",
      "--- 80.41674876213074 seconds ---\n",
      "Accuracy: 0.8254999977350235\n",
      "Epoch: 0124 Avg. cost = 74.889\n",
      "--- 80.71613621711731 seconds ---\n",
      "Accuracy: 0.8236999982595443\n",
      "Epoch: 0125 Avg. cost = 75.354\n",
      "--- 81.49902749061584 seconds ---\n",
      "Accuracy: 0.8149999970197678\n",
      "Epoch: 0126 Avg. cost = 74.621\n",
      "--- 81.07189321517944 seconds ---\n",
      "Accuracy: 0.8208999991416931\n",
      "Epoch: 0127 Avg. cost = 74.866\n",
      "--- 80.32443809509277 seconds ---\n",
      "Accuracy: 0.8214999997615814\n",
      "Epoch: 0128 Avg. cost = 74.558\n",
      "--- 81.19570755958557 seconds ---\n",
      "Accuracy: 0.82799999833107\n",
      "Epoch: 0129 Avg. cost = 74.350\n",
      "--- 80.65322041511536 seconds ---\n",
      "Accuracy: 0.828599997162819\n",
      "Epoch: 0130 Avg. cost = 73.560\n",
      "--- 80.93183994293213 seconds ---\n",
      "Accuracy: 0.8307999992370605\n",
      "Epoch: 0131 Avg. cost = 70.864\n",
      "--- 80.36213207244873 seconds ---\n",
      "Accuracy: 0.8301999998092652\n",
      "Epoch: 0132 Avg. cost = 73.811\n",
      "--- 80.75034666061401 seconds ---\n",
      "Accuracy: 0.8262999999523163\n",
      "Epoch: 0133 Avg. cost = 73.514\n",
      "--- 80.64186072349548 seconds ---\n",
      "Accuracy: 0.8210000002384186\n",
      "Epoch: 0134 Avg. cost = 70.435\n",
      "--- 80.96720242500305 seconds ---\n",
      "Accuracy: 0.8221999967098236\n",
      "Epoch: 0135 Avg. cost = 71.054\n",
      "--- 80.69485640525818 seconds ---\n",
      "Accuracy: 0.8183999979496002\n",
      "Epoch: 0136 Avg. cost = 71.233\n",
      "--- 81.0323212146759 seconds ---\n",
      "Accuracy: 0.8224999982118607\n",
      "Epoch: 0137 Avg. cost = 68.444\n",
      "--- 80.91862392425537 seconds ---\n",
      "Accuracy: 0.8236999994516373\n",
      "Epoch: 0138 Avg. cost = 69.853\n",
      "--- 80.56950569152832 seconds ---\n",
      "Accuracy: 0.8260999977588653\n",
      "Epoch: 0139 Avg. cost = 72.382\n",
      "--- 80.73569107055664 seconds ---\n",
      "Accuracy: 0.8223999977111817\n",
      "Epoch: 0140 Avg. cost = 68.724\n",
      "--- 81.2068190574646 seconds ---\n",
      "Accuracy: 0.8325999999046325\n",
      "Epoch: 0141 Avg. cost = 66.628\n",
      "--- 80.66758608818054 seconds ---\n",
      "Accuracy: 0.828299994468689\n",
      "Epoch: 0142 Avg. cost = 72.001\n",
      "--- 80.80868983268738 seconds ---\n",
      "Accuracy: 0.8333999979496002\n",
      "Epoch: 0143 Avg. cost = 67.776\n",
      "--- 80.75357341766357 seconds ---\n",
      "Accuracy: 0.8294999986886978\n",
      "Epoch: 0144 Avg. cost = 69.019\n",
      "--- 81.3538007736206 seconds ---\n",
      "Accuracy: 0.8241999977827072\n",
      "Epoch: 0145 Avg. cost = 70.032\n",
      "--- 80.56402206420898 seconds ---\n",
      "Accuracy: 0.8202999955415726\n",
      "Epoch: 0146 Avg. cost = 69.329\n",
      "--- 80.0363221168518 seconds ---\n",
      "Accuracy: 0.8201999992132187\n",
      "Epoch: 0147 Avg. cost = 67.302\n",
      "--- 81.02670431137085 seconds ---\n",
      "Accuracy: 0.8263999986648559\n",
      "Epoch: 0148 Avg. cost = 67.423\n",
      "--- 81.18316698074341 seconds ---\n",
      "Accuracy: 0.8295999979972839\n",
      "Epoch: 0149 Avg. cost = 66.546\n",
      "--- 81.00435447692871 seconds ---\n",
      "Accuracy: 0.825399996638298\n",
      "Epoch: 0150 Avg. cost = 68.648\n",
      "--- 80.89078187942505 seconds ---\n",
      "Accuracy: 0.8222999989986419\n",
      "Epoch: 0151 Avg. cost = 66.760\n",
      "--- 80.7918472290039 seconds ---\n",
      "Accuracy: 0.8242999964952469\n",
      "Epoch: 0152 Avg. cost = 63.182\n",
      "--- 80.63098931312561 seconds ---\n",
      "Accuracy: 0.8186999994516373\n",
      "Epoch: 0153 Avg. cost = 68.832\n",
      "--- 81.29551839828491 seconds ---\n",
      "Accuracy: 0.823599997162819\n",
      "Epoch: 0154 Avg. cost = 66.333\n",
      "--- 80.88604617118835 seconds ---\n",
      "Accuracy: 0.820799998641014\n",
      "Epoch: 0155 Avg. cost = 67.802\n",
      "--- 80.62625002861023 seconds ---\n",
      "Accuracy: 0.8199999976158142\n",
      "Epoch: 0156 Avg. cost = 64.934\n",
      "--- 80.38541293144226 seconds ---\n",
      "Accuracy: 0.8264999979734421\n",
      "Epoch: 0157 Avg. cost = 64.393\n",
      "--- 80.82849264144897 seconds ---\n",
      "Accuracy: 0.8217999970912934\n",
      "Epoch: 0158 Avg. cost = 64.666\n",
      "--- 80.83967304229736 seconds ---\n",
      "Accuracy: 0.8203999972343445\n",
      "Epoch: 0159 Avg. cost = 63.082\n",
      "--- 81.06568646430969 seconds ---\n",
      "Accuracy: 0.824499997496605\n",
      "Epoch: 0160 Avg. cost = 64.109\n",
      "--- 80.72109937667847 seconds ---\n",
      "Accuracy: 0.8271999967098236\n",
      "Epoch: 0161 Avg. cost = 66.087\n",
      "--- 81.40159845352173 seconds ---\n",
      "Accuracy: 0.8252999979257584\n",
      "Epoch: 0162 Avg. cost = 62.197\n",
      "--- 80.79246044158936 seconds ---\n",
      "Accuracy: 0.8201999968290329\n",
      "Epoch: 0163 Avg. cost = 64.392\n",
      "--- 80.31246590614319 seconds ---\n",
      "Accuracy: 0.8251999974250793\n",
      "Epoch: 0164 Avg. cost = 64.020\n",
      "--- 80.65665745735168 seconds ---\n",
      "Accuracy: 0.8282999962568283\n",
      "Epoch: 0165 Avg. cost = 62.091\n",
      "--- 81.51191711425781 seconds ---\n",
      "Accuracy: 0.8214999961853028\n",
      "Epoch: 0166 Avg. cost = 63.712\n",
      "--- 81.10616660118103 seconds ---\n",
      "Accuracy: 0.8226999968290329\n",
      "Epoch: 0167 Avg. cost = 62.541\n",
      "--- 80.651682138443 seconds ---\n",
      "Accuracy: 0.8239999967813492\n",
      "Epoch: 0168 Avg. cost = 62.222\n",
      "--- 80.8891773223877 seconds ---\n",
      "Accuracy: 0.8350999963283539\n",
      "Epoch: 0169 Avg. cost = 61.507\n",
      "--- 80.98484587669373 seconds ---\n",
      "Accuracy: 0.8293999969959259\n",
      "Epoch: 0170 Avg. cost = 62.971\n",
      "--- 80.73404097557068 seconds ---\n",
      "Accuracy: 0.8169999939203262\n",
      "Epoch: 0171 Avg. cost = 62.232\n",
      "--- 80.66281485557556 seconds ---\n",
      "Accuracy: 0.8252999991178512\n",
      "Epoch: 0172 Avg. cost = 61.375\n",
      "--- 80.9815571308136 seconds ---\n",
      "Accuracy: 0.8217999982833862\n",
      "Epoch: 0173 Avg. cost = 61.162\n",
      "--- 80.75155639648438 seconds ---\n",
      "Accuracy: 0.8317999988794327\n",
      "Epoch: 0174 Avg. cost = 59.334\n",
      "--- 80.811283826828 seconds ---\n",
      "Accuracy: 0.8300999969244003\n",
      "Epoch: 0175 Avg. cost = 62.411\n",
      "--- 81.14070987701416 seconds ---\n",
      "Accuracy: 0.8247999972105027\n",
      "Epoch: 0176 Avg. cost = 59.994\n",
      "--- 80.67506885528564 seconds ---\n",
      "Accuracy: 0.8145999979972839\n",
      "Epoch: 0177 Avg. cost = 62.636\n",
      "--- 81.15344548225403 seconds ---\n",
      "Accuracy: 0.8235999977588654\n",
      "Epoch: 0178 Avg. cost = 60.167\n",
      "--- 80.89844274520874 seconds ---\n",
      "Accuracy: 0.8279000002145768\n",
      "Epoch: 0179 Avg. cost = 60.667\n",
      "--- 80.79555058479309 seconds ---\n",
      "Accuracy: 0.8213999992609025\n",
      "Epoch: 0180 Avg. cost = 58.608\n",
      "--- 80.67290544509888 seconds ---\n",
      "Accuracy: 0.8219999980926513\n",
      "Epoch: 0181 Avg. cost = 60.246\n",
      "--- 80.78669023513794 seconds ---\n",
      "Accuracy: 0.8265999978780747\n",
      "Epoch: 0182 Avg. cost = 59.834\n",
      "--- 81.01183557510376 seconds ---\n",
      "Accuracy: 0.8278000003099442\n",
      "Epoch: 0183 Avg. cost = 58.948\n",
      "--- 81.42055892944336 seconds ---\n",
      "Accuracy: 0.8261999976634979\n",
      "Epoch: 0184 Avg. cost = 60.249\n",
      "--- 81.02691912651062 seconds ---\n",
      "Accuracy: 0.8314000004529953\n",
      "Epoch: 0185 Avg. cost = 60.809\n",
      "--- 80.65802597999573 seconds ---\n",
      "Accuracy: 0.8297999966144561\n",
      "Epoch: 0186 Avg. cost = 57.393\n",
      "--- 80.72997117042542 seconds ---\n",
      "Accuracy: 0.828099998831749\n",
      "Epoch: 0187 Avg. cost = 58.637\n",
      "--- 81.00409078598022 seconds ---\n",
      "Accuracy: 0.8222999966144562\n",
      "Epoch: 0188 Avg. cost = 57.704\n",
      "--- 81.34480667114258 seconds ---\n",
      "Accuracy: 0.8266999983787536\n",
      "Epoch: 0189 Avg. cost = 54.839\n",
      "--- 80.5586621761322 seconds ---\n",
      "Accuracy: 0.8249999970197678\n",
      "Epoch: 0190 Avg. cost = 58.122\n",
      "--- 81.09497308731079 seconds ---\n",
      "Accuracy: 0.8203999960422516\n",
      "Epoch: 0191 Avg. cost = 55.513\n",
      "--- 80.83449363708496 seconds ---\n",
      "Accuracy: 0.8293999999761581\n",
      "Epoch: 0192 Avg. cost = 57.477\n",
      "--- 80.7057056427002 seconds ---\n",
      "Accuracy: 0.83219999730587\n",
      "Epoch: 0193 Avg. cost = 55.347\n",
      "--- 81.35515451431274 seconds ---\n",
      "Accuracy: 0.828999999165535\n",
      "Epoch: 0194 Avg. cost = 55.972\n",
      "--- 80.66920638084412 seconds ---\n",
      "Accuracy: 0.8242999941110611\n",
      "Epoch: 0195 Avg. cost = 56.454\n",
      "--- 81.07239937782288 seconds ---\n",
      "Accuracy: 0.8237999993562698\n",
      "Epoch: 0196 Avg. cost = 57.078\n",
      "--- 80.68208956718445 seconds ---\n",
      "Accuracy: 0.8322999984025955\n",
      "Epoch: 0197 Avg. cost = 56.009\n",
      "--- 77.08777022361755 seconds ---\n",
      "Accuracy: 0.8293999981880188\n",
      "Epoch: 0198 Avg. cost = 57.088\n",
      "--- 80.67642068862915 seconds ---\n",
      "Accuracy: 0.8249999976158142\n",
      "Epoch: 0199 Avg. cost = 57.206\n",
      "--- 81.43643999099731 seconds ---\n",
      "Accuracy: 0.8270000022649765\n",
      "Epoch: 0200 Avg. cost = 54.367\n",
      "--- 80.81177830696106 seconds ---\n",
      "Accuracy: 0.8299999970197678\n",
      "Epoch: 0201 Avg. cost = 56.921\n",
      "--- 81.236492395401 seconds ---\n",
      "Accuracy: 0.8218999993801117\n",
      "Epoch: 0202 Avg. cost = 54.269\n",
      "--- 80.37132477760315 seconds ---\n",
      "Accuracy: 0.8273999989032745\n",
      "Epoch: 0203 Avg. cost = 56.863\n",
      "--- 80.91636610031128 seconds ---\n",
      "Accuracy: 0.8349999982118607\n",
      "Epoch: 0204 Avg. cost = 55.679\n",
      "--- 80.9303810596466 seconds ---\n",
      "Accuracy: 0.8213999980688095\n",
      "Epoch: 0205 Avg. cost = 55.001\n",
      "--- 80.82418179512024 seconds ---\n",
      "Accuracy: 0.8270000010728836\n",
      "Epoch: 0206 Avg. cost = 54.412\n",
      "--- 80.92379021644592 seconds ---\n",
      "Accuracy: 0.8292999970912933\n",
      "Epoch: 0207 Avg. cost = 54.962\n",
      "--- 80.96619486808777 seconds ---\n",
      "Accuracy: 0.8245999974012375\n",
      "Epoch: 0208 Avg. cost = 55.854\n",
      "--- 81.23320198059082 seconds ---\n",
      "Accuracy: 0.8206999975442887\n",
      "Epoch: 0209 Avg. cost = 54.167\n",
      "--- 80.71041107177734 seconds ---\n",
      "Accuracy: 0.8299999988079071\n",
      "Epoch: 0210 Avg. cost = 52.827\n",
      "--- 81.0742871761322 seconds ---\n",
      "Accuracy: 0.8272999948263169\n",
      "Epoch: 0211 Avg. cost = 55.756\n",
      "--- 80.85228872299194 seconds ---\n",
      "Accuracy: 0.8252999991178512\n",
      "Epoch: 0212 Avg. cost = 54.489\n",
      "--- 80.58588552474976 seconds ---\n",
      "Accuracy: 0.8269999969005585\n",
      "Epoch: 0213 Avg. cost = 54.185\n",
      "--- 80.82229089736938 seconds ---\n",
      "Accuracy: 0.8290000003576279\n",
      "Epoch: 0214 Avg. cost = 54.522\n",
      "--- 80.53261017799377 seconds ---\n",
      "Accuracy: 0.8283000010251999\n",
      "Epoch: 0215 Avg. cost = 52.382\n",
      "--- 81.24155354499817 seconds ---\n",
      "Accuracy: 0.825\n",
      "Epoch: 0216 Avg. cost = 54.348\n",
      "--- 81.12042212486267 seconds ---\n",
      "Accuracy: 0.8263999950885773\n",
      "Epoch: 0217 Avg. cost = 52.631\n",
      "--- 81.104327917099 seconds ---\n",
      "Accuracy: 0.8282999932765961\n",
      "Epoch: 0218 Avg. cost = 55.203\n",
      "--- 80.69394612312317 seconds ---\n",
      "Accuracy: 0.8215999966859817\n",
      "Epoch: 0219 Avg. cost = 51.697\n",
      "--- 81.0883436203003 seconds ---\n",
      "Accuracy: 0.8283999973535537\n",
      "Epoch: 0220 Avg. cost = 51.645\n",
      "--- 80.81047296524048 seconds ---\n",
      "Accuracy: 0.8264999973773957\n",
      "Epoch: 0221 Avg. cost = 53.611\n",
      "--- 81.05944395065308 seconds ---\n",
      "Accuracy: 0.8257999992370606\n",
      "Epoch: 0222 Avg. cost = 52.334\n",
      "--- 81.04109954833984 seconds ---\n",
      "Accuracy: 0.8337999987602234\n",
      "Epoch: 0223 Avg. cost = 52.175\n",
      "--- 80.94906568527222 seconds ---\n",
      "Accuracy: 0.8264000004529953\n",
      "Epoch: 0224 Avg. cost = 53.114\n",
      "--- 80.94742393493652 seconds ---\n",
      "Accuracy: 0.8212000000476837\n",
      "Epoch: 0225 Avg. cost = 52.278\n",
      "--- 80.98207759857178 seconds ---\n",
      "Accuracy: 0.8225999969244003\n",
      "Epoch: 0226 Avg. cost = 50.728\n",
      "--- 80.7386543750763 seconds ---\n",
      "Accuracy: 0.8246999979019165\n",
      "Epoch: 0227 Avg. cost = 53.806\n",
      "--- 81.43391489982605 seconds ---\n",
      "Accuracy: 0.8301999974250793\n",
      "Epoch: 0228 Avg. cost = 50.689\n",
      "--- 81.26420998573303 seconds ---\n",
      "Accuracy: 0.8387999945878982\n",
      "Epoch: 0229 Avg. cost = 52.066\n",
      "--- 81.41540431976318 seconds ---\n",
      "Accuracy: 0.8276999980211258\n",
      "Epoch: 0230 Avg. cost = 51.868\n",
      "--- 80.69520878791809 seconds ---\n",
      "Accuracy: 0.8351999986171722\n",
      "Epoch: 0231 Avg. cost = 49.980\n",
      "--- 80.87052392959595 seconds ---\n",
      "Accuracy: 0.8288999992609024\n",
      "Epoch: 0232 Avg. cost = 50.436\n",
      "--- 81.30596852302551 seconds ---\n",
      "Accuracy: 0.8225000005960464\n",
      "Epoch: 0233 Avg. cost = 50.661\n",
      "--- 80.85303330421448 seconds ---\n",
      "Accuracy: 0.8262999963760376\n",
      "Epoch: 0234 Avg. cost = 50.332\n",
      "--- 80.73082327842712 seconds ---\n",
      "Accuracy: 0.8261999976634979\n",
      "Epoch: 0235 Avg. cost = 50.687\n",
      "--- 80.97440528869629 seconds ---\n",
      "Accuracy: 0.8308999979496002\n",
      "Epoch: 0236 Avg. cost = 51.765\n",
      "--- 80.89879369735718 seconds ---\n",
      "Accuracy: 0.8242999958992004\n",
      "Epoch: 0237 Avg. cost = 51.068\n",
      "--- 80.52230501174927 seconds ---\n",
      "Accuracy: 0.8297000002861022\n",
      "Epoch: 0238 Avg. cost = 49.372\n",
      "--- 81.10573744773865 seconds ---\n",
      "Accuracy: 0.8299999988079071\n",
      "Epoch: 0239 Avg. cost = 50.143\n",
      "--- 81.07752394676208 seconds ---\n",
      "Accuracy: 0.8275999993085861\n",
      "Epoch: 0240 Avg. cost = 50.832\n",
      "--- 80.71040797233582 seconds ---\n",
      "Accuracy: 0.8311999982595444\n",
      "Epoch: 0241 Avg. cost = 51.384\n",
      "--- 81.136301279068 seconds ---\n",
      "Accuracy: 0.8321999990940094\n",
      "Epoch: 0242 Avg. cost = 50.029\n",
      "--- 81.02202105522156 seconds ---\n",
      "Accuracy: 0.8312999975681304\n",
      "Epoch: 0243 Avg. cost = 47.307\n",
      "--- 80.91449856758118 seconds ---\n",
      "Accuracy: 0.8288999956846237\n",
      "Epoch: 0244 Avg. cost = 50.257\n",
      "--- 81.16709208488464 seconds ---\n",
      "Accuracy: 0.8266999983787536\n",
      "Epoch: 0245 Avg. cost = 48.843\n",
      "--- 80.43034934997559 seconds ---\n",
      "Accuracy: 0.824799998998642\n",
      "Epoch: 0246 Avg. cost = 49.379\n",
      "--- 80.68181872367859 seconds ---\n",
      "Accuracy: 0.8358999997377395\n",
      "Epoch: 0247 Avg. cost = 49.696\n",
      "--- 80.61882996559143 seconds ---\n",
      "Accuracy: 0.825900000333786\n",
      "Epoch: 0248 Avg. cost = 50.070\n",
      "--- 80.69906067848206 seconds ---\n",
      "Accuracy: 0.8245000004768371\n",
      "Epoch: 0249 Avg. cost = 48.758\n",
      "--- 80.89240169525146 seconds ---\n",
      "Accuracy: 0.8305999982357025\n",
      "Epoch: 0250 Avg. cost = 48.550\n",
      "--- 80.84978103637695 seconds ---\n",
      "Accuracy: 0.8309999990463257\n",
      "Epoch: 0251 Avg. cost = 48.917\n",
      "--- 81.25699877738953 seconds ---\n",
      "Accuracy: 0.8310999989509582\n",
      "Epoch: 0252 Avg. cost = 49.517\n",
      "--- 80.63682889938354 seconds ---\n",
      "Accuracy: 0.8266999995708466\n",
      "Epoch: 0253 Avg. cost = 49.132\n",
      "--- 80.509042263031 seconds ---\n",
      "Accuracy: 0.8240999978780746\n",
      "Epoch: 0254 Avg. cost = 49.481\n",
      "--- 80.70542049407959 seconds ---\n",
      "Accuracy: 0.8315999948978424\n",
      "Epoch: 0255 Avg. cost = 48.277\n",
      "--- 80.85696053504944 seconds ---\n",
      "Accuracy: 0.8307999974489212\n",
      "Epoch: 0256 Avg. cost = 47.530\n",
      "--- 80.88492107391357 seconds ---\n",
      "Accuracy: 0.8301999968290329\n",
      "Epoch: 0257 Avg. cost = 48.449\n",
      "--- 80.65883994102478 seconds ---\n",
      "Accuracy: 0.8249999964237213\n",
      "Epoch: 0258 Avg. cost = 47.730\n",
      "--- 80.74722456932068 seconds ---\n",
      "Accuracy: 0.8291999983787537\n",
      "Epoch: 0259 Avg. cost = 47.178\n",
      "--- 80.4796073436737 seconds ---\n",
      "Accuracy: 0.8283999985456467\n",
      "Epoch: 0260 Avg. cost = 47.759\n",
      "--- 81.01107239723206 seconds ---\n",
      "Accuracy: 0.8270999997854233\n",
      "Epoch: 0261 Avg. cost = 48.059\n",
      "--- 80.77106642723083 seconds ---\n",
      "Accuracy: 0.8319999974966049\n",
      "Epoch: 0262 Avg. cost = 48.736\n",
      "--- 80.82202100753784 seconds ---\n",
      "Accuracy: 0.8329999989271164\n",
      "Epoch: 0263 Avg. cost = 47.950\n",
      "--- 81.13825702667236 seconds ---\n",
      "Accuracy: 0.825799998641014\n",
      "Epoch: 0264 Avg. cost = 46.619\n",
      "--- 80.97903227806091 seconds ---\n",
      "Accuracy: 0.8245999979972839\n",
      "Epoch: 0265 Avg. cost = 48.201\n",
      "--- 80.82275080680847 seconds ---\n",
      "Accuracy: 0.834599996805191\n",
      "Epoch: 0266 Avg. cost = 45.620\n",
      "--- 80.77689027786255 seconds ---\n",
      "Accuracy: 0.833099998831749\n",
      "Epoch: 0267 Avg. cost = 47.984\n",
      "--- 80.97523546218872 seconds ---\n",
      "Accuracy: 0.8365999990701676\n",
      "Epoch: 0268 Avg. cost = 46.639\n",
      "--- 80.58079266548157 seconds ---\n",
      "Accuracy: 0.8242999982833862\n",
      "Epoch: 0269 Avg. cost = 45.570\n",
      "--- 80.93402171134949 seconds ---\n",
      "Accuracy: 0.82769999563694\n",
      "Epoch: 0270 Avg. cost = 48.304\n",
      "--- 81.09538340568542 seconds ---\n",
      "Accuracy: 0.830699999332428\n",
      "Epoch: 0271 Avg. cost = 44.570\n",
      "--- 80.70623016357422 seconds ---\n",
      "Accuracy: 0.8245999985933303\n",
      "Epoch: 0272 Avg. cost = 45.111\n",
      "--- 80.60661149024963 seconds ---\n",
      "Accuracy: 0.8360999989509582\n",
      "Epoch: 0273 Avg. cost = 46.790\n",
      "--- 80.41302847862244 seconds ---\n",
      "Accuracy: 0.8291999965906143\n",
      "Epoch: 0274 Avg. cost = 47.000\n",
      "--- 81.11427164077759 seconds ---\n",
      "Accuracy: 0.8302999961376191\n",
      "Epoch: 0275 Avg. cost = 44.719\n",
      "--- 80.60878801345825 seconds ---\n",
      "Accuracy: 0.8229999989271164\n",
      "Epoch: 0276 Avg. cost = 45.213\n",
      "--- 80.69791221618652 seconds ---\n",
      "Accuracy: 0.8284999966621399\n",
      "Epoch: 0277 Avg. cost = 45.493\n",
      "--- 80.88665723800659 seconds ---\n",
      "Accuracy: 0.8237999993562698\n",
      "Epoch: 0278 Avg. cost = 44.615\n",
      "--- 80.58049964904785 seconds ---\n",
      "Accuracy: 0.8243999969959259\n",
      "Epoch: 0279 Avg. cost = 47.777\n",
      "--- 80.99480724334717 seconds ---\n",
      "Accuracy: 0.8212999993562698\n",
      "Epoch: 0280 Avg. cost = 43.244\n",
      "--- 81.2344183921814 seconds ---\n",
      "Accuracy: 0.8286999976634979\n",
      "Epoch: 0281 Avg. cost = 46.049\n",
      "--- 80.43695116043091 seconds ---\n",
      "Accuracy: 0.8266999971866608\n",
      "Epoch: 0282 Avg. cost = 46.255\n",
      "--- 81.06039524078369 seconds ---\n",
      "Accuracy: 0.8194999986886978\n",
      "Epoch: 0283 Avg. cost = 45.797\n",
      "--- 80.77692174911499 seconds ---\n",
      "Accuracy: 0.8292999982833862\n",
      "Epoch: 0284 Avg. cost = 45.352\n",
      "--- 81.2512104511261 seconds ---\n",
      "Accuracy: 0.8313999962806702\n",
      "Epoch: 0285 Avg. cost = 46.662\n",
      "--- 80.91585350036621 seconds ---\n",
      "Accuracy: 0.8244999969005584\n",
      "Epoch: 0286 Avg. cost = 46.078\n",
      "--- 80.6094286441803 seconds ---\n",
      "Accuracy: 0.8291999995708466\n",
      "Epoch: 0287 Avg. cost = 44.444\n",
      "--- 81.01706504821777 seconds ---\n",
      "Accuracy: 0.8272999954223633\n",
      "Epoch: 0288 Avg. cost = 46.039\n",
      "--- 81.74895572662354 seconds ---\n",
      "Accuracy: 0.8306999963521957\n",
      "Epoch: 0289 Avg. cost = 45.848\n",
      "--- 80.7565667629242 seconds ---\n",
      "Accuracy: 0.8356000006198883\n",
      "Epoch: 0290 Avg. cost = 43.483\n",
      "--- 80.62118148803711 seconds ---\n",
      "Accuracy: 0.8307999980449676\n",
      "Epoch: 0291 Avg. cost = 43.576\n",
      "--- 80.98790550231934 seconds ---\n",
      "Accuracy: 0.8267999976873398\n",
      "Epoch: 0292 Avg. cost = 46.192\n",
      "--- 80.9957447052002 seconds ---\n",
      "Accuracy: 0.8332999974489212\n",
      "Epoch: 0293 Avg. cost = 45.797\n",
      "--- 80.82319808006287 seconds ---\n",
      "Accuracy: 0.8315999984741211\n",
      "Epoch: 0294 Avg. cost = 43.403\n",
      "--- 80.79239082336426 seconds ---\n",
      "Accuracy: 0.829599996805191\n",
      "Epoch: 0295 Avg. cost = 43.396\n",
      "--- 81.13341808319092 seconds ---\n",
      "Accuracy: 0.8244999986886978\n",
      "Epoch: 0296 Avg. cost = 45.395\n",
      "--- 81.00485563278198 seconds ---\n",
      "Accuracy: 0.8364999979734421\n",
      "Epoch: 0297 Avg. cost = 45.882\n",
      "--- 80.77751469612122 seconds ---\n",
      "Accuracy: 0.8294999998807907\n",
      "Epoch: 0298 Avg. cost = 43.663\n",
      "--- 81.59511876106262 seconds ---\n",
      "Accuracy: 0.8319999986886978\n",
      "Epoch: 0299 Avg. cost = 44.022\n",
      "--- 80.80584573745728 seconds ---\n",
      "Accuracy: 0.8262999993562699\n",
      "Epoch: 0300 Avg. cost = 44.571\n",
      "--- 80.54068565368652 seconds ---\n",
      "Accuracy: 0.8352999979257584\n",
      "Epoch: 0301 Avg. cost = 43.026\n",
      "--- 80.46977472305298 seconds ---\n",
      "Accuracy: 0.8290999972820282\n",
      "Epoch: 0302 Avg. cost = 43.243\n",
      "--- 81.4629020690918 seconds ---\n",
      "Accuracy: 0.8338999962806701\n",
      "Epoch: 0303 Avg. cost = 43.696\n",
      "--- 81.54073762893677 seconds ---\n",
      "Accuracy: 0.8306999987363816\n",
      "Epoch: 0304 Avg. cost = 44.384\n",
      "--- 80.65122246742249 seconds ---\n",
      "Accuracy: 0.8279999965429305\n",
      "Epoch: 0305 Avg. cost = 42.496\n",
      "--- 81.37928295135498 seconds ---\n",
      "Accuracy: 0.8304999995231629\n",
      "Epoch: 0306 Avg. cost = 43.820\n",
      "--- 81.3783814907074 seconds ---\n",
      "Accuracy: 0.8244999980926514\n",
      "Epoch: 0307 Avg. cost = 43.357\n",
      "--- 80.96586322784424 seconds ---\n",
      "Accuracy: 0.8303999948501587\n",
      "Epoch: 0308 Avg. cost = 43.266\n",
      "--- 81.22177219390869 seconds ---\n",
      "Accuracy: 0.8328999990224838\n",
      "Epoch: 0309 Avg. cost = 44.628\n",
      "--- 80.86209487915039 seconds ---\n",
      "Accuracy: 0.8293999993801117\n",
      "Epoch: 0310 Avg. cost = 42.927\n",
      "--- 81.07402014732361 seconds ---\n",
      "Accuracy: 0.8202999961376191\n",
      "Epoch: 0311 Avg. cost = 42.526\n",
      "--- 81.08962917327881 seconds ---\n",
      "Accuracy: 0.8328999978303909\n",
      "Epoch: 0312 Avg. cost = 44.186\n",
      "--- 81.25824213027954 seconds ---\n",
      "Accuracy: 0.8227999991178513\n",
      "Epoch: 0313 Avg. cost = 42.420\n",
      "--- 80.64828991889954 seconds ---\n",
      "Accuracy: 0.8311999988555908\n",
      "Epoch: 0314 Avg. cost = 41.363\n",
      "--- 80.74497270584106 seconds ---\n",
      "Accuracy: 0.8264999955892562\n",
      "Epoch: 0315 Avg. cost = 42.075\n",
      "--- 81.12843775749207 seconds ---\n",
      "Accuracy: 0.8316999977827072\n",
      "Epoch: 0316 Avg. cost = 42.961\n",
      "--- 80.66198420524597 seconds ---\n",
      "Accuracy: 0.8297999978065491\n",
      "Epoch: 0317 Avg. cost = 40.624\n",
      "--- 80.38264226913452 seconds ---\n",
      "Accuracy: 0.8320999991893768\n",
      "Epoch: 0318 Avg. cost = 42.204\n",
      "--- 80.26058435440063 seconds ---\n",
      "Accuracy: 0.8301999986171722\n",
      "Epoch: 0319 Avg. cost = 42.981\n",
      "--- 81.04333209991455 seconds ---\n",
      "Accuracy: 0.8315000003576278\n",
      "Epoch: 0320 Avg. cost = 42.899\n",
      "--- 81.15818166732788 seconds ---\n",
      "Accuracy: 0.8282999968528748\n",
      "Epoch: 0321 Avg. cost = 41.743\n",
      "--- 81.02589583396912 seconds ---\n",
      "Accuracy: 0.8282999986410141\n",
      "Epoch: 0322 Avg. cost = 41.075\n",
      "--- 80.81124496459961 seconds ---\n",
      "Accuracy: 0.8334999972581864\n",
      "Epoch: 0323 Avg. cost = 41.719\n",
      "--- 80.96475315093994 seconds ---\n",
      "Accuracy: 0.8271999990940094\n",
      "Epoch: 0324 Avg. cost = 42.689\n",
      "--- 81.16845202445984 seconds ---\n",
      "Accuracy: 0.8255999982357025\n",
      "Epoch: 0325 Avg. cost = 42.939\n",
      "--- 80.30890274047852 seconds ---\n",
      "Accuracy: 0.8305000007152558\n",
      "Epoch: 0326 Avg. cost = 41.223\n",
      "--- 81.85121297836304 seconds ---\n",
      "Accuracy: 0.8262999993562699\n",
      "Epoch: 0327 Avg. cost = 40.856\n",
      "--- 81.3636748790741 seconds ---\n",
      "Accuracy: 0.8331999987363815\n",
      "Epoch: 0328 Avg. cost = 41.407\n",
      "--- 80.88588166236877 seconds ---\n",
      "Accuracy: 0.8355000001192093\n",
      "Epoch: 0329 Avg. cost = 42.145\n",
      "--- 80.83640599250793 seconds ---\n",
      "Accuracy: 0.8337000000476837\n",
      "Epoch: 0330 Avg. cost = 42.890\n",
      "--- 81.54398775100708 seconds ---\n",
      "Accuracy: 0.831099995970726\n",
      "Epoch: 0331 Avg. cost = 43.448\n",
      "--- 80.83755922317505 seconds ---\n",
      "Accuracy: 0.8249999970197678\n",
      "Epoch: 0332 Avg. cost = 42.178\n",
      "--- 80.00993394851685 seconds ---\n",
      "Accuracy: 0.8371999996900559\n",
      "Epoch: 0333 Avg. cost = 39.538\n",
      "--- 80.79561400413513 seconds ---\n",
      "Accuracy: 0.8306000000238418\n",
      "Epoch: 0334 Avg. cost = 41.574\n",
      "--- 80.98576188087463 seconds ---\n",
      "Accuracy: 0.8321999967098236\n",
      "Epoch: 0335 Avg. cost = 41.328\n",
      "--- 80.89125633239746 seconds ---\n",
      "Accuracy: 0.8269999998807908\n",
      "Epoch: 0336 Avg. cost = 41.464\n",
      "--- 80.49762392044067 seconds ---\n",
      "Accuracy: 0.8264999967813492\n",
      "Epoch: 0337 Avg. cost = 41.186\n",
      "--- 80.8231840133667 seconds ---\n",
      "Accuracy: 0.8322999948263168\n",
      "Epoch: 0338 Avg. cost = 38.602\n",
      "--- 81.3874762058258 seconds ---\n",
      "Accuracy: 0.826099995970726\n",
      "Epoch: 0339 Avg. cost = 37.702\n",
      "--- 80.88629794120789 seconds ---\n",
      "Accuracy: 0.8313999962806702\n",
      "Epoch: 0340 Avg. cost = 41.624\n",
      "--- 80.64552927017212 seconds ---\n",
      "Accuracy: 0.8278999978303909\n",
      "Epoch: 0341 Avg. cost = 41.119\n",
      "--- 81.52653455734253 seconds ---\n",
      "Accuracy: 0.8290999978780746\n",
      "Epoch: 0342 Avg. cost = 39.883\n",
      "--- 81.38716626167297 seconds ---\n",
      "Accuracy: 0.8314999985694885\n",
      "Epoch: 0343 Avg. cost = 40.162\n",
      "--- 80.86793994903564 seconds ---\n",
      "Accuracy: 0.8336999988555909\n",
      "Epoch: 0344 Avg. cost = 41.600\n",
      "--- 80.73058557510376 seconds ---\n",
      "Accuracy: 0.8266999965906143\n",
      "Epoch: 0345 Avg. cost = 39.356\n",
      "--- 81.12928605079651 seconds ---\n",
      "Accuracy: 0.8295999979972839\n",
      "Epoch: 0346 Avg. cost = 41.358\n",
      "--- 80.80863070487976 seconds ---\n",
      "Accuracy: 0.8346999990940094\n",
      "Epoch: 0347 Avg. cost = 40.070\n",
      "--- 80.96134829521179 seconds ---\n",
      "Accuracy: 0.8337999993562698\n",
      "Epoch: 0348 Avg. cost = 39.012\n",
      "--- 81.16248488426208 seconds ---\n",
      "Accuracy: 0.824899998307228\n",
      "Epoch: 0349 Avg. cost = 39.520\n",
      "--- 80.85450673103333 seconds ---\n",
      "Accuracy: 0.8308999997377395\n",
      "Epoch: 0350 Avg. cost = 39.369\n",
      "--- 80.8537700176239 seconds ---\n",
      "Accuracy: 0.8373000007867814\n",
      "Epoch: 0351 Avg. cost = 39.178\n",
      "--- 80.5169997215271 seconds ---\n",
      "Accuracy: 0.837199998497963\n",
      "Epoch: 0352 Avg. cost = 40.986\n",
      "--- 81.10528087615967 seconds ---\n",
      "Accuracy: 0.8272999978065491\n",
      "Epoch: 0353 Avg. cost = 39.550\n",
      "--- 80.73046112060547 seconds ---\n",
      "Accuracy: 0.8332999992370606\n",
      "Epoch: 0354 Avg. cost = 39.099\n",
      "--- 81.15602231025696 seconds ---\n",
      "Accuracy: 0.8271999996900559\n",
      "Epoch: 0355 Avg. cost = 41.756\n",
      "--- 81.42346143722534 seconds ---\n",
      "Accuracy: 0.825499997138977\n",
      "Epoch: 0356 Avg. cost = 40.137\n",
      "--- 80.60556650161743 seconds ---\n",
      "Accuracy: 0.8292999970912933\n",
      "Epoch: 0357 Avg. cost = 39.900\n",
      "--- 80.65280032157898 seconds ---\n",
      "Accuracy: 0.830799993276596\n",
      "Epoch: 0358 Avg. cost = 39.870\n",
      "--- 80.80175614356995 seconds ---\n",
      "Accuracy: 0.8367999970912934\n",
      "Epoch: 0359 Avg. cost = 38.282\n",
      "--- 81.056640625 seconds ---\n",
      "Accuracy: 0.8346999973058701\n",
      "Epoch: 0360 Avg. cost = 38.585\n",
      "--- 80.51177215576172 seconds ---\n",
      "Accuracy: 0.8357999986410141\n",
      "Epoch: 0361 Avg. cost = 39.579\n",
      "--- 80.99187994003296 seconds ---\n",
      "Accuracy: 0.8357999974489212\n",
      "Epoch: 0362 Avg. cost = 38.693\n",
      "--- 81.02904319763184 seconds ---\n",
      "Accuracy: 0.8298999971151352\n",
      "Epoch: 0363 Avg. cost = 37.573\n",
      "--- 80.57603907585144 seconds ---\n",
      "Accuracy: 0.833599997162819\n",
      "Epoch: 0364 Avg. cost = 40.538\n",
      "--- 80.58251214027405 seconds ---\n",
      "Accuracy: 0.8296999979019165\n",
      "Epoch: 0365 Avg. cost = 39.298\n",
      "--- 80.56159687042236 seconds ---\n",
      "Accuracy: 0.8267999994754791\n",
      "Epoch: 0366 Avg. cost = 39.307\n",
      "--- 81.11999845504761 seconds ---\n",
      "Accuracy: 0.8272999978065491\n",
      "Epoch: 0367 Avg. cost = 38.203\n",
      "--- 80.63948154449463 seconds ---\n",
      "Accuracy: 0.8355999964475632\n",
      "Epoch: 0368 Avg. cost = 38.999\n",
      "--- 80.54611730575562 seconds ---\n",
      "Accuracy: 0.8297999984025956\n",
      "Epoch: 0369 Avg. cost = 38.687\n",
      "--- 81.18660259246826 seconds ---\n",
      "Accuracy: 0.831499999165535\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-cb63ea9c01c7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_batch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_one_hot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcost_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mphase\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[0mtotal_cost\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mcost_val\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Loss function\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_))\n",
    "\n",
    "train_batch_size = 100\n",
    "test_batch_size = 100\n",
    "total_train_batch = int(50000/train_batch_size)\n",
    "total_test_batch = int(10000/test_batch_size)\n",
    "\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "\n",
    "number_of_epochs = 10000\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "with tf.control_dependencies(update_ops):\n",
    "     # Ensures that we execute the update_ops before performing the train_step\n",
    "     train_step = tf.train.AdamOptimizer(0.001).minimize(cost, global_step=global_step)\n",
    "    \n",
    "\n",
    "sess = tf.InteractiveSession() \n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for epoch in range(number_of_epochs):\n",
    "    total_cost = 0\n",
    "    total_accuracy = 0\n",
    "#    if ckpt:\n",
    "#        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "#    else:\n",
    "#        sess.run(tf.global_variables_initializer())\n",
    "    start_time = time.time()\n",
    "    for i in range(total_train_batch):\n",
    "        \n",
    "        batch = next_batch(train_batch_size, x_train, y_train_one_hot.eval())\n",
    "        _, cost_val = sess.run([train_step,cost], feed_dict={x: batch[0], y_: batch[1], phase: True})\n",
    "        total_cost += cost_val\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1),'Avg. cost =', '{:.3f}'.format(total_cost))\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    \n",
    "    for i in range(total_test_batch):\n",
    "\n",
    "        test_batch = next_batch(test_batch_size, x_test, y_test_one_hot.eval()) \n",
    "        acc = sess.run(accuracy, feed_dict={x: test_batch[0], y_: test_batch[1], phase: False})\n",
    "        total_accuracy += acc\n",
    "    \n",
    "    mean_accuracy = total_accuracy/total_test_batch\n",
    "    print('Accuracy:', mean_accuracy) # Mean accuracy for test batches\n",
    "    \n",
    "# save weights as checkpoint file.    \n",
    "#    saver.save(sess, './cnn(cifar10)/dnn.ckpt', global_step=global_step)\n",
    "\n",
    "\n",
    "print('End of optimization!')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
