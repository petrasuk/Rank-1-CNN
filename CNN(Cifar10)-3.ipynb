{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#본 코드는 VGG-16을 Rank-1으로 구현한 것으로 VGG-16의 모델은 밑의 파일을 기반으로 작성함.\n",
    "\n",
    "\n",
    "https://github.com/petrasuk/tiny_imagenet/blob/master/src/vgg_16.py\n",
    "\n",
    "Cifar10 예제는 다음을 참조함.\n",
    "http://solarisailab.com/archives/2325"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal CNN \n",
    "\n",
    "cifar 10 data set에 대해서 함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 나중에 다시 고쳐보기\n",
    "\n",
    "# assume input_op shape is 224x224x3 \n",
    "\n",
    " \n",
    "# block 1 -- outputs 112x112x64 \n",
    "\n",
    "convolution 2번 후에 max pooling\n",
    "\n",
    " \n",
    "# block 2 -- outputs 56x56x128 \n",
    "convolution 2번 후에 max pooling\n",
    "\n",
    " \n",
    "# block 3 -- outputs 28x28x256 \n",
    "convolution 2번 후에 max pooling\n",
    " \n",
    "# block 4 -- outputs 14x14x512 \n",
    "convolution 3번 후에 max pooling\n",
    "\n",
    "\n",
    "# block 5 -- outputs 7x7x512 \n",
    "convolution 3번 후에 max pooling\n",
    "\n",
    "# flatten 1\n",
    "flatten 후 --> 4096\n",
    "\n",
    "\n",
    "# flatten 2\n",
    "flatten 후 --> 2048\n",
    "\n",
    " \n",
    "# fully connected \n",
    "fc6 = fc_op(resh1, name=\"fc6\", n_out=1024) \n",
    "fc6_drop = tf.nn.dropout(fc6, dropout_keep_prob, name=\"fc6_drop\") \n",
    "fc7 = fc_op(fc6_drop, name=\"fc7\", n_out=1024) \n",
    "fc7_drop = tf.nn.dropout(fc7, dropout_keep_prob, name=\"fc7_drop\") \n",
    "fc8 = fc_op(fc7_drop, name=\"fc8\", n_out=10) \n",
    "   \n",
    "softmax = tf.nn.softmax(fc8) \n",
    "predictions = tf.argmax(softmax, 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import re\n",
    "import os\n",
    "\n",
    "# use matplotlib as inline\n",
    "%matplotlib inline \n",
    "from functools import partial\n",
    "from matplotlib.pyplot import *\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.keras._impl.keras.datasets.cifar10 import load_data\n",
    "\n",
    "# 다음 배치를 읽어오기 위한 next_batch 유틸리티 함수를 정의합니다. \n",
    "def next_batch(num, data, labels): \n",
    "    ''' \n",
    "    Return a total of `num` random samples and labels.  \n",
    "    ''' \n",
    "    idx = np.arange(0 , len(data)) \n",
    "    np.random.shuffle(idx) \n",
    "    idx = idx[:num] \n",
    "    data_shuffle = [data[i] for i in idx] \n",
    "    labels_shuffle = [labels[i] for i in idx] \n",
    "\n",
    " \n",
    "    return np.asarray(data_shuffle), np.asarray(labels_shuffle) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " #GPU를 90%까지 쓸 수 있기 위해 설정함.  \n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allocator_type ='BFC'\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(\"float\", [None, 32, 32, 3])\n",
    "#x2 = tf.image.resize_images(x, [224, 224])\n",
    "phase = tf.placeholder(tf.bool, name='phase') #batch normalization을 training모드인지 test모드인지\n",
    "\n",
    "# conv1_1\n",
    "input_num = 3\n",
    "dim = 3 # 1-rank 1D filter의 한쪽 사이즈\n",
    "num = 8 # 1-rank 1D filter의 갯수. 8개이면 2개의 1D filter에 의해 2D filter는 64개가 된다.\n",
    "output_num = num*num\n",
    "\n",
    "#W1_1 = tf.Variable(tf.random_uniform([3,3,input_num,output_num], -0.5, 0.5))\n",
    "#W1_1 = tf.Variable(tf.random_uniform([3,3,input_num,output_num], -0.5, 0.5))\n",
    "W1_1 = tf.get_variable(\"W\", shape=[3,3,input_num,output_num],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "\n",
    "L1 = tf.nn.conv2d(x, W1_1, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "Bias1_1 = tf.Variable(tf.constant(0.0, shape=[output_num], dtype=tf.float32),\n",
    "                                 trainable=True, name='Bias1_1')\n",
    "L1 = tf.nn.bias_add(L1, Bias1_1)\n",
    "L1 = tf.nn.relu(L1)\n",
    "# batch normalization\n",
    "L1 = tf.layers.batch_normalization(L1, training=phase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3, 3, 64)\n"
     ]
    }
   ],
   "source": [
    "print(W1_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# conv1_2\n",
    "input_num = 64\n",
    "dim = 3 # 1-rank 1D filter의 한쪽 사이즈\n",
    "num = 8 # 1-rank 1D filter의 갯수. 8개이면 2개의 1D filter에 의해 2D filter는 64개가 된다.\n",
    "output_num = num*num\n",
    "\n",
    "#W1_2 = tf.Variable(tf.random_uniform([3,3,input_num,output_num], -0.5, 0.5))\n",
    "W1_2 = tf.get_variable(\"W1_2\", shape=[3,3,input_num,output_num],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "L1 = tf.nn.conv2d(L1, W1_2, strides=[1, 1, 1, 1], padding='SAME')\n",
    "Bias1_2 = tf.Variable(tf.constant(0.0, shape=[output_num], dtype=tf.float32),\n",
    "                                 trainable=True, name='Bias1_2')\n",
    "L1 = tf.nn.bias_add(L1, Bias1_2)\n",
    "\n",
    "L1 = tf.nn.relu(L1)\n",
    "# Max_Pool1\n",
    "# 입력 : (N, 160, 160, 64)  --> 출력 : (N, 80, 80, 64)\n",
    "L1 = tf.nn.max_pool(L1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "###########################dropout 추가\n",
    "\n",
    "L1 = tf.nn.dropout(L1, 0.5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 16, 16, 64)\n"
     ]
    }
   ],
   "source": [
    "print(L1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# conv2_1\n",
    "# 입력 : (N, 80, 80, 64)  --> 출력 : (N, 80, 80, 144)\n",
    "# 커널 : ( [3x1]크기 P  12개 x Q [1x3]크기 12개)= [3x3]크기 2차원 행렬 144개 x T 144개 = 3차원 행렬 144개\n",
    "\n",
    "input_num = 64\n",
    "dim = 3 # 1-rank 1D filter의 한쪽 사이즈\n",
    "num = 12 # 1-rank 1D filter의 갯수. 8개이면 2개의 1D filter에 의해 2D filter는 64개가 된다.\n",
    "output_num = num*num\n",
    "\n",
    "#W2_1 = tf.Variable(tf.random_uniform([3,3,input_num,output_num], -0.5, 0.5))\n",
    "W2_1 = tf.get_variable(\"W2_1\", shape=[3,3,input_num,output_num],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "L2 = tf.nn.conv2d(L1, W2_1, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "Bias2_1 = tf.Variable(tf.constant(0.0, shape=[output_num], dtype=tf.float32),\n",
    "                                 trainable=True, name='Bias2_1')\n",
    "L2 = tf.nn.bias_add(L2, Bias2_1)\n",
    "L2 = tf.nn.relu(L2)\n",
    "\n",
    "# batch normalization\n",
    "L2 = tf.layers.batch_normalization(L2, training=phase)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 16, 16, 144)\n"
     ]
    }
   ],
   "source": [
    "print(L2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# conv2_2\n",
    "# 입력 : (N, 80, 80, 64)  --> 출력 : (N, 80, 80, 144)\n",
    "# 커널 : ( [3x1]크기 P  12개 x Q [1x3]크기 12개)= [3x3]크기 2차원 행렬 144개 x T 144개 = 3차원 행렬 144개\n",
    "\n",
    "input_num = 144\n",
    "dim = 3 # 1-rank 1D filter의 한쪽 사이즈\n",
    "num = 12 # 1-rank 1D filter의 갯수. 8개이면 2개의 1D filter에 의해 2D filter는 64개가 된다.\n",
    "output_num = num*num\n",
    "\n",
    "#W2_2 = tf.Variable(tf.random_uniform([3,3,input_num,output_num], -0.5, 0.5))\n",
    "W2_2 = tf.get_variable(\"W2_2\", shape=[3,3,input_num,output_num],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "#L2 = tf.Variable(tf.random_uniform([None, 224, 224, 1], -0.5, 0.5))\n",
    "\n",
    "L2 = tf.nn.conv2d(L2, W2_2, strides=[1, 1, 1, 1], padding='SAME')\n",
    "Bias2_2 = tf.Variable(tf.constant(0.0, shape=[output_num], dtype=tf.float32),\n",
    "                                 trainable=True, name='Bias2_2')\n",
    "L2 = tf.nn.bias_add(L2, Bias2_2)\n",
    "L2 = tf.nn.relu(L2)\n",
    "\n",
    "L2 = tf.nn.max_pool(L2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "L2 = tf.nn.dropout(L2, 0.5) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 8, 8, 144)\n"
     ]
    }
   ],
   "source": [
    "print(L2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# conv3_1\n",
    "# 입력 : (N, 40, 40, 144)  --> 출력 : (N, 40, 40, 256)\n",
    "# 커널 : ( [3x1]크기 P 16개 x Q [1x3]크기 16개)= [3x3]크기 2차원 행렬 256개 x T 256개 = 3차원 행렬 256개\n",
    "\n",
    "input_num = 144\n",
    "dim = 3 # 1-rank 1D filter의 한쪽 사이즈\n",
    "num = 16 # 1-rank 1D filter의 갯수. 8개이면 2개의 1D filter에 의해 2D filter는 64개가 된다.\n",
    "output_num = num*num\n",
    "\n",
    "#W3_1 = tf.Variable(tf.random_uniform([3,3,input_num,output_num], -0.5, 0.5))\n",
    "W3_1 = tf.get_variable(\"W3_1\", shape=[3,3,input_num,output_num],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "L3 = tf.nn.conv2d(L2, W3_1, strides=[1, 1, 1, 1], padding='SAME')\n",
    "Bias3_1 = tf.Variable(tf.constant(0.0, shape=[output_num], dtype=tf.float32),\n",
    "                                 trainable=True, name='Bias3_1')\n",
    "L3 = tf.nn.bias_add(L3, Bias3_1)\n",
    "L3 = tf.nn.relu(L3)\n",
    "\n",
    "# batch normalization\n",
    "L3 = tf.layers.batch_normalization(L3, training=phase)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 8, 8, 256)\n"
     ]
    }
   ],
   "source": [
    "print(L3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# conv3_2\n",
    "# 입력 : (N, 40, 40, 144)  --> 출력 : (N, 40, 40, 256)\n",
    "# 커널 : ( [3x1]크기 P 16개 x Q [1x3]크기 16개)= [3x3]크기 2차원 행렬 256개 x T 256개 = 3차원 행렬 256개\n",
    "\n",
    "input_num = 256\n",
    "dim = 3\n",
    "num = 16 # 1-rank 1D filter의 갯수. 8개이면 2개의 1D filter에 의해 2D filter는 64개가 된다.\n",
    "output_num = num*num\n",
    "\n",
    "#W3_2 = tf.Variable(tf.random_uniform([3,3,input_num,output_num], -0.5, 0.5))\n",
    "W3_2 = tf.get_variable(\"W3_2\", shape=[3,3,input_num,output_num],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "L3 = tf.nn.conv2d(L3, W3_2, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "Bias3_2 = tf.Variable(tf.constant(0.0, shape=[output_num], dtype=tf.float32),\n",
    "                                 trainable=True, name='Bias3_2')\n",
    "L3 = tf.nn.bias_add(L3, Bias3_2)\n",
    "L3 = tf.nn.relu(L3)\n",
    "\n",
    "L3 = tf.nn.max_pool(L3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "L3 = tf.nn.dropout(L3, 0.5) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 4, 4, 256)\n"
     ]
    }
   ],
   "source": [
    "print(L3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dense(inputs, units, name=None): \n",
    "#\"\"\"3x3 conv layer: ReLU + He initialization\"\"\" \n",
    "# He initialization: normal dist with stdev = sqrt(2.0/fan-in) \n",
    "    stddev = np.sqrt(2 / int(inputs.shape[1])) \n",
    "    out = tf.layers.dense(inputs, units, \n",
    "                           kernel_initializer=tf.contrib.layers.xavier_initializer(), \n",
    "                           kernel_regularizer=tf.contrib.layers.l2_regularizer(1.0), \n",
    "                           name=name) \n",
    "\n",
    "    return out \n",
    "\n",
    "def dense_relu(inputs, units, name=None): \n",
    "    # \"\"\"3x3 conv layer: ReLU + He initialization\"\"\"   \n",
    "    # He initialization: normal dist with stdev = sqrt(2.0/fanin) \n",
    "    stddev = np.sqrt(2 / int(inputs.shape[1])) \n",
    "    out = tf.layers.dense(inputs, units, activation=tf.nn.relu, \n",
    "                         kernel_initializer=tf.contrib.layers.xavier_initializer(), \n",
    "                         kernel_regularizer=tf.contrib.layers.l2_regularizer(1.0), \n",
    "                         name=name) \n",
    "  \n",
    "    return out \n",
    "\n",
    "def dense_batch_relu(x, output, phase, scope):\n",
    "    with tf.variable_scope(scope):\n",
    "        h1 = tf.contrib.layers.fully_connected(x, output, \n",
    "                                               activation_fn=None,\n",
    "                                               scope='dense')\n",
    "        h2 = tf.contrib.layers.batch_norm(h1, \n",
    "                                          center=True, scale=True, \n",
    "                                          is_training=phase,\n",
    "                                          scope='bn')\n",
    "        return tf.nn.relu(h2, 'relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_ = tf.placeholder(\"float\", [None,10])\n",
    "\n",
    "out = tf.contrib.layers.flatten(L3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fc1: flatten -> fully connected layer \n",
    "#   (N, 5, 5, 484) -> (N, 12,100) -> (N, 4096) \n",
    "#   out = tf.contrib.layers.flatten(out) \n",
    "#   out = dense_relu(out, 4096, 'fc1') \n",
    "#   out = tf.nn.dropout(out, config.dropout_keep_prob) \n",
    "# fc2 \n",
    "# (N, 4096) -> (N, 2048) \n",
    "# fc3 \n",
    "# (N, 2048) -> (N, 2) \n",
    "\n",
    "out = dense_batch_relu(out, 1024, phase,'fc1')\n",
    "\n",
    "#fc1b = tf.Variable(tf.constant(1.0, shape=[1024], dtype=tf.float32),\n",
    "#                                 trainable=True, name='fc1b')\n",
    "#out = tf.nn.bias_add(out, fc1b)\n",
    "out = tf.nn.dropout(out, 0.5) \n",
    "out = dense_batch_relu(out, 512, phase,'fc2')\n",
    "\n",
    "#fc2b = tf.Variable(tf.constant(1.0, shape=[1024], dtype=tf.float32),\n",
    "#                                 trainable=True, name='fc2b')\n",
    "#out = tf.nn.bias_add(out, fc2b)\n",
    "\n",
    "out = tf.nn.dropout(out, 0.5) \n",
    "\n",
    "class_number = 10\n",
    "logits = dense(out, class_number, 'fc3')  # class number =2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CIFAR-10 데이터를 다운로드하고 데이터를 불러옵니다. \n",
    "#def normalize(X_train,X_test): \n",
    "        #this function normalize inputs for zero mean and unit variance \n",
    "        # it is used when training a model. \n",
    "        # Input: training set and test set \n",
    "        # Output: normalized training set and test set according to the trianing set statistics. \n",
    "#        mean = np.mean(X_train,axis=(0,1,2,3)) \n",
    "#        std = np.std(X_train, axis=(0, 1, 2, 3)) \n",
    "#        X_train = (X_train-mean)/(std+1e-7) \n",
    "#        X_test = (X_test-mean)/(std+1e-7) \n",
    "#        return X_train, X_test \n",
    "\n",
    "#(x_train, y_train), (x_test, y_test) = load_data() \n",
    "#x_train = x_train.astype('float32') \n",
    "#x_test = x_test.astype('float32') \n",
    "#x_train, x_test = normalize(x_train, x_test) \n",
    "\n",
    "def normalize(X_train,X_test): \n",
    "        #this function normalize inputs for zero mean and unit variance \n",
    "        # it is used when training a model. \n",
    "        # Input: training set and test set \n",
    "        # Output: normalized training set and test set according to the trianing set statistics. \n",
    " #       mean = np.mean(X_train,axis=(0,1,2,3)) \n",
    "#        X_train = ((X_train/255.0)-0.5)*4.0\n",
    "#        X_test = ((X_test/255.0)-0.5)*4.0\n",
    "        X_train = X_train-128\n",
    "        X_test =  X_test-128\n",
    "\n",
    "#        std = np.std(X_train, axis=(0, 1, 2, 3)) \n",
    "#        X_train = (X_train-mean)/(std+1e-7) \n",
    "#        X_test = (X_test-mean)/(std+1e-7) \n",
    "        return X_train, X_test \n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = load_data() \n",
    "x_train = x_train.astype('float32') \n",
    "x_test = x_test.astype('float32') \n",
    "x_train, x_test = normalize(x_train, x_test) \n",
    "\n",
    "# scalar 형태의 레이블(0~9)을 One-hot Encoding 형태로 변환합니다. \n",
    "y_train_one_hot = tf.squeeze(tf.one_hot(y_train, 10),axis=1) \n",
    "y_test_one_hot = tf.squeeze(tf.one_hot(y_test, 10),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x18600026a90>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(y_train[1000])\n",
    "plt.imshow(x_train[1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#########\n",
    "# Result\n",
    "########\n",
    "is_correct = tf.equal(tf.argmax(logits, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-22-ee3d9b124e2d>:2: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "Epoch: 0001 Avg. cost = 831.354\n",
      "--- 18.224003553390503 seconds ---\n",
      "Accuracy: 0.3795999997854233\n",
      "Epoch: 0002 Avg. cost = 549.396\n",
      "--- 16.579060792922974 seconds ---\n",
      "Accuracy: 0.5556999963521957\n",
      "Epoch: 0003 Avg. cost = 449.665\n",
      "--- 16.744030475616455 seconds ---\n",
      "Accuracy: 0.6531000006198883\n",
      "Epoch: 0004 Avg. cost = 391.021\n",
      "--- 16.843279123306274 seconds ---\n",
      "Accuracy: 0.7079999995231628\n",
      "Epoch: 0005 Avg. cost = 350.213\n",
      "--- 16.75106716156006 seconds ---\n",
      "Accuracy: 0.7252000004053116\n",
      "Epoch: 0006 Avg. cost = 317.275\n",
      "--- 16.708984851837158 seconds ---\n",
      "Accuracy: 0.7464000016450882\n",
      "Epoch: 0007 Avg. cost = 290.833\n",
      "--- 16.796111345291138 seconds ---\n",
      "Accuracy: 0.7547000014781952\n",
      "Epoch: 0008 Avg. cost = 270.735\n",
      "--- 16.76414179801941 seconds ---\n",
      "Accuracy: 0.768400001525879\n",
      "Epoch: 0009 Avg. cost = 256.882\n",
      "--- 16.677074909210205 seconds ---\n",
      "Accuracy: 0.7797999972105026\n",
      "Epoch: 0010 Avg. cost = 242.226\n",
      "--- 16.833312511444092 seconds ---\n",
      "Accuracy: 0.7789000010490418\n",
      "Epoch: 0011 Avg. cost = 229.078\n",
      "--- 16.80930781364441 seconds ---\n",
      "Accuracy: 0.796399998664856\n",
      "Epoch: 0012 Avg. cost = 216.854\n",
      "--- 16.841224431991577 seconds ---\n",
      "Accuracy: 0.7916000002622604\n",
      "Epoch: 0013 Avg. cost = 204.847\n",
      "--- 16.731043815612793 seconds ---\n",
      "Accuracy: 0.7974999994039536\n",
      "Epoch: 0014 Avg. cost = 189.987\n",
      "--- 16.60199737548828 seconds ---\n",
      "Accuracy: 0.7931999975442886\n",
      "Epoch: 0015 Avg. cost = 191.387\n",
      "--- 16.52104139328003 seconds ---\n",
      "Accuracy: 0.8068999975919724\n",
      "Epoch: 0016 Avg. cost = 180.430\n",
      "--- 16.384032249450684 seconds ---\n",
      "Accuracy: 0.8018000006675721\n",
      "Epoch: 0017 Avg. cost = 173.399\n",
      "--- 16.72521948814392 seconds ---\n",
      "Accuracy: 0.7965999954938888\n",
      "Epoch: 0018 Avg. cost = 165.774\n",
      "--- 16.78508472442627 seconds ---\n",
      "Accuracy: 0.8039999979734421\n",
      "Epoch: 0019 Avg. cost = 160.790\n",
      "--- 16.77212166786194 seconds ---\n",
      "Accuracy: 0.8108000004291535\n",
      "Epoch: 0020 Avg. cost = 154.666\n",
      "--- 16.68813157081604 seconds ---\n",
      "Accuracy: 0.8156999987363815\n",
      "Epoch: 0021 Avg. cost = 151.989\n",
      "--- 16.419994592666626 seconds ---\n",
      "Accuracy: 0.8179000020027161\n",
      "Epoch: 0022 Avg. cost = 146.019\n",
      "--- 16.538994550704956 seconds ---\n",
      "Accuracy: 0.812399999499321\n",
      "Epoch: 0023 Avg. cost = 142.926\n",
      "--- 16.655264139175415 seconds ---\n",
      "Accuracy: 0.8123999971151352\n",
      "Epoch: 0024 Avg. cost = 135.297\n",
      "--- 16.78311586380005 seconds ---\n",
      "Accuracy: 0.8166999995708466\n",
      "Epoch: 0025 Avg. cost = 132.353\n",
      "--- 16.7691969871521 seconds ---\n",
      "Accuracy: 0.8280999982357025\n",
      "Epoch: 0026 Avg. cost = 130.045\n",
      "--- 16.720064640045166 seconds ---\n",
      "Accuracy: 0.8200999963283538\n",
      "Epoch: 0027 Avg. cost = 124.232\n",
      "--- 16.626142501831055 seconds ---\n",
      "Accuracy: 0.8223999983072281\n",
      "Epoch: 0028 Avg. cost = 123.594\n",
      "--- 16.715065240859985 seconds ---\n",
      "Accuracy: 0.817399999499321\n",
      "Epoch: 0029 Avg. cost = 118.835\n",
      "--- 16.691056489944458 seconds ---\n",
      "Accuracy: 0.83\n",
      "Epoch: 0030 Avg. cost = 117.063\n",
      "--- 16.737306594848633 seconds ---\n",
      "Accuracy: 0.8262000000476837\n",
      "Epoch: 0031 Avg. cost = 112.402\n",
      "--- 16.76336646080017 seconds ---\n",
      "Accuracy: 0.8301999986171722\n",
      "Epoch: 0032 Avg. cost = 109.808\n",
      "--- 16.66458749771118 seconds ---\n",
      "Accuracy: 0.8223999989032745\n",
      "Epoch: 0033 Avg. cost = 107.941\n",
      "--- 16.51310968399048 seconds ---\n",
      "Accuracy: 0.819799998998642\n",
      "Epoch: 0034 Avg. cost = 103.463\n",
      "--- 16.779158115386963 seconds ---\n",
      "Accuracy: 0.828099998831749\n",
      "Epoch: 0035 Avg. cost = 101.340\n",
      "--- 16.76213026046753 seconds ---\n",
      "Accuracy: 0.83\n",
      "Epoch: 0036 Avg. cost = 98.437\n",
      "--- 16.758243799209595 seconds ---\n",
      "Accuracy: 0.8303999996185303\n",
      "Epoch: 0037 Avg. cost = 94.900\n",
      "--- 16.75019884109497 seconds ---\n",
      "Accuracy: 0.8210999941825867\n",
      "Epoch: 0038 Avg. cost = 97.400\n",
      "--- 16.744105100631714 seconds ---\n",
      "Accuracy: 0.8282999992370605\n",
      "Epoch: 0039 Avg. cost = 91.935\n",
      "--- 16.792943239212036 seconds ---\n",
      "Accuracy: 0.8356999981403351\n",
      "Epoch: 0040 Avg. cost = 92.710\n",
      "--- 16.792996644973755 seconds ---\n",
      "Accuracy: 0.8393999952077865\n",
      "Epoch: 0041 Avg. cost = 90.980\n",
      "--- 16.91109848022461 seconds ---\n",
      "Accuracy: 0.8201999986171722\n",
      "Epoch: 0042 Avg. cost = 86.229\n",
      "--- 16.79919195175171 seconds ---\n",
      "Accuracy: 0.8338000011444092\n",
      "Epoch: 0043 Avg. cost = 85.645\n",
      "--- 16.812328338623047 seconds ---\n",
      "Accuracy: 0.8423000013828278\n",
      "Epoch: 0044 Avg. cost = 85.709\n",
      "--- 16.73213005065918 seconds ---\n",
      "Accuracy: 0.8302999955415725\n",
      "Epoch: 0045 Avg. cost = 81.732\n",
      "--- 16.64007544517517 seconds ---\n",
      "Accuracy: 0.8333999961614609\n",
      "Epoch: 0046 Avg. cost = 83.212\n",
      "--- 16.80409860610962 seconds ---\n",
      "Accuracy: 0.832399999499321\n",
      "Epoch: 0047 Avg. cost = 81.055\n",
      "--- 16.754000663757324 seconds ---\n",
      "Accuracy: 0.8320000010728836\n",
      "Epoch: 0048 Avg. cost = 78.657\n",
      "--- 16.654030799865723 seconds ---\n",
      "Accuracy: 0.8370999962091445\n",
      "Epoch: 0049 Avg. cost = 76.549\n",
      "--- 16.5889949798584 seconds ---\n",
      "Accuracy: 0.8420999985933304\n",
      "Epoch: 0050 Avg. cost = 75.978\n",
      "--- 16.7739896774292 seconds ---\n",
      "Accuracy: 0.828600001335144\n",
      "Epoch: 0051 Avg. cost = 75.843\n",
      "--- 16.669206619262695 seconds ---\n",
      "Accuracy: 0.8291999965906143\n",
      "Epoch: 0052 Avg. cost = 74.700\n",
      "--- 16.64812159538269 seconds ---\n",
      "Accuracy: 0.8374000000953674\n",
      "Epoch: 0053 Avg. cost = 74.016\n",
      "--- 16.64002513885498 seconds ---\n",
      "Accuracy: 0.8293000012636185\n",
      "Epoch: 0054 Avg. cost = 70.929\n",
      "--- 16.685089588165283 seconds ---\n",
      "Accuracy: 0.8320000004768372\n",
      "Epoch: 0055 Avg. cost = 69.316\n",
      "--- 16.74201726913452 seconds ---\n",
      "Accuracy: 0.8383000004291534\n",
      "Epoch: 0056 Avg. cost = 69.569\n",
      "--- 16.647108554840088 seconds ---\n",
      "Accuracy: 0.8351999986171722\n",
      "Epoch: 0057 Avg. cost = 68.008\n",
      "--- 16.815133094787598 seconds ---\n",
      "Accuracy: 0.8388999980688095\n",
      "Epoch: 0058 Avg. cost = 68.677\n",
      "--- 16.743242502212524 seconds ---\n",
      "Accuracy: 0.8384999978542328\n",
      "Epoch: 0059 Avg. cost = 66.776\n",
      "--- 16.8281307220459 seconds ---\n",
      "Accuracy: 0.8338999968767166\n",
      "Epoch: 0060 Avg. cost = 65.815\n",
      "--- 16.785076379776 seconds ---\n",
      "Accuracy: 0.8431000000238419\n",
      "Epoch: 0061 Avg. cost = 66.684\n",
      "--- 16.762084484100342 seconds ---\n",
      "Accuracy: 0.8417999970912934\n",
      "Epoch: 0062 Avg. cost = 62.606\n",
      "--- 16.79516625404358 seconds ---\n",
      "Accuracy: 0.8359999978542327\n",
      "Epoch: 0063 Avg. cost = 62.449\n",
      "--- 16.6041316986084 seconds ---\n",
      "Accuracy: 0.8326999974250794\n",
      "Epoch: 0064 Avg. cost = 62.502\n",
      "--- 16.665076971054077 seconds ---\n",
      "Accuracy: 0.8391999977827073\n",
      "Epoch: 0065 Avg. cost = 61.073\n",
      "--- 16.805097818374634 seconds ---\n",
      "Accuracy: 0.839899999499321\n",
      "Epoch: 0066 Avg. cost = 59.666\n",
      "--- 16.3489933013916 seconds ---\n",
      "Accuracy: 0.844300000667572\n",
      "Epoch: 0067 Avg. cost = 60.032\n",
      "--- 16.44506525993347 seconds ---\n",
      "Accuracy: 0.8401999962329865\n",
      "Epoch: 0068 Avg. cost = 60.238\n",
      "--- 16.47713041305542 seconds ---\n",
      "Accuracy: 0.834899998307228\n",
      "Epoch: 0069 Avg. cost = 58.562\n",
      "--- 16.7810275554657 seconds ---\n",
      "Accuracy: 0.8441999953985214\n",
      "Epoch: 0070 Avg. cost = 58.360\n",
      "--- 16.714233875274658 seconds ---\n",
      "Accuracy: 0.8435999995470047\n",
      "Epoch: 0071 Avg. cost = 56.718\n",
      "--- 16.844327211380005 seconds ---\n",
      "Accuracy: 0.8416000002622605\n",
      "Epoch: 0072 Avg. cost = 57.997\n",
      "--- 16.758996725082397 seconds ---\n",
      "Accuracy: 0.8446999949216842\n",
      "Epoch: 0073 Avg. cost = 55.732\n",
      "--- 16.7301025390625 seconds ---\n",
      "Accuracy: 0.8390999954938888\n",
      "Epoch: 0074 Avg. cost = 53.265\n",
      "--- 16.8400981426239 seconds ---\n",
      "Accuracy: 0.8458999991416931\n",
      "Epoch: 0075 Avg. cost = 55.081\n",
      "--- 16.79931139945984 seconds ---\n",
      "Accuracy: 0.8431000000238419\n",
      "Epoch: 0076 Avg. cost = 53.795\n",
      "--- 16.761982679367065 seconds ---\n",
      "Accuracy: 0.8445000004768372\n",
      "Epoch: 0077 Avg. cost = 53.435\n",
      "--- 16.775978326797485 seconds ---\n",
      "Accuracy: 0.8514000004529954\n",
      "Epoch: 0078 Avg. cost = 51.464\n",
      "--- 16.709205865859985 seconds ---\n",
      "Accuracy: 0.8393999981880188\n",
      "Epoch: 0079 Avg. cost = 51.568\n",
      "--- 16.774024724960327 seconds ---\n",
      "Accuracy: 0.8443000012636185\n",
      "Epoch: 0080 Avg. cost = 52.536\n",
      "--- 16.70101761817932 seconds ---\n",
      "Accuracy: 0.8402999985218048\n",
      "Epoch: 0081 Avg. cost = 51.093\n",
      "--- 16.73109221458435 seconds ---\n",
      "Accuracy: 0.8369999980926514\n",
      "Epoch: 0082 Avg. cost = 52.770\n",
      "--- 16.79910683631897 seconds ---\n",
      "Accuracy: 0.8466999995708465\n",
      "Epoch: 0083 Avg. cost = 49.792\n",
      "--- 16.668166637420654 seconds ---\n",
      "Accuracy: 0.8498999959230423\n",
      "Epoch: 0084 Avg. cost = 49.078\n",
      "--- 16.72107696533203 seconds ---\n",
      "Accuracy: 0.8416999971866608\n",
      "Epoch: 0085 Avg. cost = 48.486\n",
      "--- 16.952348709106445 seconds ---\n",
      "Accuracy: 0.8365999978780746\n",
      "Epoch: 0086 Avg. cost = 49.483\n",
      "--- 16.799132347106934 seconds ---\n",
      "Accuracy: 0.8381999981403351\n",
      "Epoch: 0087 Avg. cost = 47.760\n",
      "--- 16.671358108520508 seconds ---\n",
      "Accuracy: 0.8455999952554702\n",
      "Epoch: 0088 Avg. cost = 49.488\n",
      "--- 16.779994010925293 seconds ---\n",
      "Accuracy: 0.8468999987840653\n",
      "Epoch: 0089 Avg. cost = 45.790\n",
      "--- 16.682106256484985 seconds ---\n",
      "Accuracy: 0.8394000011682511\n",
      "Epoch: 0090 Avg. cost = 46.648\n",
      "--- 16.698095083236694 seconds ---\n",
      "Accuracy: 0.8382999974489213\n",
      "Epoch: 0091 Avg. cost = 45.305\n",
      "--- 16.844196557998657 seconds ---\n",
      "Accuracy: 0.8435000014305115\n",
      "Epoch: 0092 Avg. cost = 45.556\n",
      "--- 16.804994821548462 seconds ---\n",
      "Accuracy: 0.8348999959230423\n",
      "Epoch: 0093 Avg. cost = 48.119\n",
      "--- 16.77507448196411 seconds ---\n",
      "Accuracy: 0.8336999982595443\n",
      "Epoch: 0094 Avg. cost = 45.635\n",
      "--- 16.8541898727417 seconds ---\n",
      "Accuracy: 0.8441999989748001\n",
      "Epoch: 0095 Avg. cost = 45.735\n",
      "--- 16.765327215194702 seconds ---\n",
      "Accuracy: 0.8418999975919723\n",
      "Epoch: 0096 Avg. cost = 43.485\n",
      "--- 16.695247411727905 seconds ---\n",
      "Accuracy: 0.8382999980449677\n",
      "Epoch: 0097 Avg. cost = 45.074\n",
      "--- 16.778182983398438 seconds ---\n",
      "Accuracy: 0.8488999998569489\n",
      "Epoch: 0098 Avg. cost = 45.335\n",
      "--- 16.760995864868164 seconds ---\n",
      "Accuracy: 0.8454000002145767\n",
      "Epoch: 0099 Avg. cost = 42.224\n",
      "--- 16.647090196609497 seconds ---\n",
      "Accuracy: 0.8393999969959259\n",
      "Epoch: 0100 Avg. cost = 44.111\n",
      "--- 16.772083044052124 seconds ---\n",
      "Accuracy: 0.8390999984741211\n",
      "Epoch: 0101 Avg. cost = 44.368\n",
      "--- 16.669291257858276 seconds ---\n",
      "Accuracy: 0.8494999992847443\n",
      "Epoch: 0102 Avg. cost = 41.611\n",
      "--- 16.701431035995483 seconds ---\n",
      "Accuracy: 0.8457999974489212\n",
      "Epoch: 0103 Avg. cost = 43.151\n",
      "--- 16.773287534713745 seconds ---\n",
      "Accuracy: 0.8471000009775161\n",
      "Epoch: 0104 Avg. cost = 41.468\n",
      "--- 16.785059690475464 seconds ---\n",
      "Accuracy: 0.8410999977588653\n",
      "Epoch: 0105 Avg. cost = 40.765\n",
      "--- 16.639994859695435 seconds ---\n",
      "Accuracy: 0.8463999998569488\n",
      "Epoch: 0106 Avg. cost = 42.946\n",
      "--- 16.56099772453308 seconds ---\n",
      "Accuracy: 0.8467000013589859\n",
      "Epoch: 0107 Avg. cost = 41.804\n",
      "--- 16.57818365097046 seconds ---\n",
      "Accuracy: 0.845299997329712\n",
      "Epoch: 0108 Avg. cost = 38.345\n",
      "--- 16.81209683418274 seconds ---\n",
      "Accuracy: 0.8477000021934509\n",
      "Epoch: 0109 Avg. cost = 40.471\n",
      "--- 16.821301221847534 seconds ---\n",
      "Accuracy: 0.8457999974489212\n",
      "Epoch: 0110 Avg. cost = 40.193\n",
      "--- 16.763185262680054 seconds ---\n",
      "Accuracy: 0.8514000010490418\n",
      "Epoch: 0111 Avg. cost = 40.494\n",
      "--- 16.799999952316284 seconds ---\n",
      "Accuracy: 0.843099997639656\n",
      "Epoch: 0112 Avg. cost = 40.434\n",
      "--- 16.726160526275635 seconds ---\n",
      "Accuracy: 0.8406999969482422\n",
      "Epoch: 0113 Avg. cost = 39.465\n",
      "--- 16.805205583572388 seconds ---\n",
      "Accuracy: 0.8474000000953674\n",
      "Epoch: 0114 Avg. cost = 38.742\n",
      "--- 16.724244832992554 seconds ---\n",
      "Accuracy: 0.8466999977827072\n",
      "Epoch: 0115 Avg. cost = 37.443\n",
      "--- 16.669137239456177 seconds ---\n",
      "Accuracy: 0.844099999666214\n",
      "Epoch: 0116 Avg. cost = 40.257\n",
      "--- 16.720128774642944 seconds ---\n",
      "Accuracy: 0.8464999997615814\n",
      "Epoch: 0117 Avg. cost = 40.674\n",
      "--- 16.840060472488403 seconds ---\n",
      "Accuracy: 0.8475999969244004\n",
      "Epoch: 0118 Avg. cost = 37.530\n",
      "--- 16.664344310760498 seconds ---\n",
      "Accuracy: 0.8466000032424926\n",
      "Epoch: 0119 Avg. cost = 37.144\n",
      "--- 16.657000303268433 seconds ---\n",
      "Accuracy: 0.8426999986171723\n",
      "Epoch: 0120 Avg. cost = 39.487\n",
      "--- 16.736217260360718 seconds ---\n",
      "Accuracy: 0.8440999984741211\n",
      "Epoch: 0121 Avg. cost = 38.261\n",
      "--- 16.739206314086914 seconds ---\n",
      "Accuracy: 0.8497000020742417\n",
      "Epoch: 0122 Avg. cost = 36.913\n",
      "--- 16.695104598999023 seconds ---\n",
      "Accuracy: 0.8521000003814697\n",
      "Epoch: 0123 Avg. cost = 37.871\n",
      "--- 16.7139949798584 seconds ---\n",
      "Accuracy: 0.8505999982357025\n",
      "Epoch: 0124 Avg. cost = 37.197\n",
      "--- 16.795141220092773 seconds ---\n",
      "Accuracy: 0.8489999985694885\n",
      "Epoch: 0125 Avg. cost = 37.866\n",
      "--- 16.75814461708069 seconds ---\n",
      "Accuracy: 0.8508999997377396\n",
      "Epoch: 0126 Avg. cost = 36.093\n",
      "--- 16.786250114440918 seconds ---\n",
      "Accuracy: 0.8494000017642975\n",
      "Epoch: 0127 Avg. cost = 35.402\n",
      "--- 16.687082052230835 seconds ---\n",
      "Accuracy: 0.8440000039339065\n",
      "Epoch: 0128 Avg. cost = 36.163\n",
      "--- 16.667125940322876 seconds ---\n",
      "Accuracy: 0.8390999990701675\n",
      "Epoch: 0129 Avg. cost = 34.921\n",
      "--- 16.842999696731567 seconds ---\n",
      "Accuracy: 0.8527000027894974\n",
      "Epoch: 0130 Avg. cost = 35.337\n",
      "--- 16.684075593948364 seconds ---\n",
      "Accuracy: 0.8474999982118606\n",
      "Epoch: 0131 Avg. cost = 35.101\n",
      "--- 16.731988430023193 seconds ---\n",
      "Accuracy: 0.8538999968767166\n",
      "Epoch: 0132 Avg. cost = 36.430\n",
      "--- 16.652059078216553 seconds ---\n",
      "Accuracy: 0.8511000007390976\n",
      "Epoch: 0133 Avg. cost = 34.466\n",
      "--- 16.64203405380249 seconds ---\n",
      "Accuracy: 0.8532000011205674\n",
      "Epoch: 0134 Avg. cost = 33.934\n",
      "--- 16.781018018722534 seconds ---\n",
      "Accuracy: 0.8387999999523162\n",
      "Epoch: 0135 Avg. cost = 36.838\n",
      "--- 16.797020435333252 seconds ---\n",
      "Accuracy: 0.8416999971866608\n",
      "Epoch: 0136 Avg. cost = 35.350\n",
      "--- 16.7439968585968 seconds ---\n",
      "Accuracy: 0.8538000011444091\n",
      "Epoch: 0137 Avg. cost = 32.572\n",
      "--- 16.7210795879364 seconds ---\n",
      "Accuracy: 0.8456999987363816\n",
      "Epoch: 0138 Avg. cost = 32.778\n",
      "--- 16.7100932598114 seconds ---\n",
      "Accuracy: 0.8532999980449677\n",
      "Epoch: 0139 Avg. cost = 35.585\n",
      "--- 16.616170644760132 seconds ---\n",
      "Accuracy: 0.8468999969959259\n",
      "Epoch: 0140 Avg. cost = 35.937\n",
      "--- 16.656943798065186 seconds ---\n",
      "Accuracy: 0.8504999971389771\n",
      "Epoch: 0141 Avg. cost = 31.583\n",
      "--- 16.719157695770264 seconds ---\n",
      "Accuracy: 0.8521999990940095\n",
      "Epoch: 0142 Avg. cost = 33.362\n",
      "--- 16.73711395263672 seconds ---\n",
      "Accuracy: 0.8551999986171722\n",
      "Epoch: 0143 Avg. cost = 34.629\n",
      "--- 16.849231243133545 seconds ---\n",
      "Accuracy: 0.8494999974966049\n",
      "Epoch: 0144 Avg. cost = 31.819\n",
      "--- 16.717999935150146 seconds ---\n",
      "Accuracy: 0.8508999985456467\n",
      "Epoch: 0145 Avg. cost = 33.362\n",
      "--- 16.757214307785034 seconds ---\n",
      "Accuracy: 0.8451999998092652\n",
      "Epoch: 0146 Avg. cost = 33.530\n",
      "--- 16.38201642036438 seconds ---\n",
      "Accuracy: 0.8442999970912933\n",
      "Epoch: 0147 Avg. cost = 32.515\n",
      "--- 16.439980268478394 seconds ---\n",
      "Accuracy: 0.848099997639656\n",
      "Epoch: 0148 Avg. cost = 31.314\n",
      "--- 16.82440710067749 seconds ---\n",
      "Accuracy: 0.8474000012874603\n",
      "Epoch: 0149 Avg. cost = 33.932\n",
      "--- 16.597994089126587 seconds ---\n",
      "Accuracy: 0.8535999983549118\n",
      "Epoch: 0150 Avg. cost = 32.160\n",
      "--- 16.78207540512085 seconds ---\n",
      "Accuracy: 0.8455999970436097\n",
      "Epoch: 0151 Avg. cost = 32.936\n",
      "--- 16.712048292160034 seconds ---\n",
      "Accuracy: 0.8518000000715256\n",
      "Epoch: 0152 Avg. cost = 31.671\n",
      "--- 16.62528109550476 seconds ---\n",
      "Accuracy: 0.8468999963998795\n",
      "Epoch: 0153 Avg. cost = 32.232\n",
      "--- 16.757014513015747 seconds ---\n",
      "Accuracy: 0.8555999982357025\n",
      "Epoch: 0154 Avg. cost = 32.213\n",
      "--- 16.761000394821167 seconds ---\n",
      "Accuracy: 0.8504999989271164\n",
      "Epoch: 0155 Avg. cost = 29.734\n",
      "--- 16.840990781784058 seconds ---\n",
      "Accuracy: 0.8485000026226044\n",
      "Epoch: 0156 Avg. cost = 32.355\n",
      "--- 16.708143949508667 seconds ---\n",
      "Accuracy: 0.8487999993562698\n",
      "Epoch: 0157 Avg. cost = 32.292\n",
      "--- 16.759094953536987 seconds ---\n",
      "Accuracy: 0.8526999998092651\n",
      "Epoch: 0158 Avg. cost = 30.726\n",
      "--- 16.69099497795105 seconds ---\n",
      "Accuracy: 0.8464999997615814\n",
      "Epoch: 0159 Avg. cost = 30.537\n",
      "--- 16.641184091567993 seconds ---\n",
      "Accuracy: 0.8532000005245208\n",
      "Epoch: 0160 Avg. cost = 29.768\n",
      "--- 16.760056972503662 seconds ---\n",
      "Accuracy: 0.8549000000953675\n",
      "Epoch: 0161 Avg. cost = 30.213\n",
      "--- 16.739376068115234 seconds ---\n",
      "Accuracy: 0.8538000029325485\n",
      "Epoch: 0162 Avg. cost = 31.084\n",
      "--- 16.819021224975586 seconds ---\n",
      "Accuracy: 0.848500000834465\n",
      "Epoch: 0163 Avg. cost = 29.297\n",
      "--- 16.674059629440308 seconds ---\n",
      "Accuracy: 0.8575999987125397\n",
      "Epoch: 0164 Avg. cost = 28.590\n",
      "--- 16.827216386795044 seconds ---\n",
      "Accuracy: 0.8472000014781952\n",
      "Epoch: 0165 Avg. cost = 29.379\n",
      "--- 16.812023401260376 seconds ---\n",
      "Accuracy: 0.8478999972343445\n",
      "Epoch: 0166 Avg. cost = 29.809\n",
      "--- 16.624130964279175 seconds ---\n",
      "Accuracy: 0.8557999992370605\n",
      "Epoch: 0167 Avg. cost = 29.173\n",
      "--- 16.57016611099243 seconds ---\n",
      "Accuracy: 0.8513000011444092\n",
      "Epoch: 0168 Avg. cost = 29.193\n",
      "--- 16.782236337661743 seconds ---\n",
      "Accuracy: 0.8550999987125397\n",
      "Epoch: 0169 Avg. cost = 30.015\n",
      "--- 16.768198251724243 seconds ---\n",
      "Accuracy: 0.8519999992847442\n",
      "Epoch: 0170 Avg. cost = 28.792\n",
      "--- 16.56215810775757 seconds ---\n",
      "Accuracy: 0.8524000000953674\n",
      "Epoch: 0171 Avg. cost = 29.879\n",
      "--- 16.713998794555664 seconds ---\n",
      "Accuracy: 0.8539999979734421\n",
      "Epoch: 0172 Avg. cost = 28.468\n",
      "--- 16.654093980789185 seconds ---\n",
      "Accuracy: 0.8539999973773956\n",
      "Epoch: 0173 Avg. cost = 27.353\n",
      "--- 16.746999979019165 seconds ---\n",
      "Accuracy: 0.8496999990940094\n",
      "Epoch: 0174 Avg. cost = 29.146\n",
      "--- 16.844127893447876 seconds ---\n",
      "Accuracy: 0.8509000009298324\n",
      "Epoch: 0175 Avg. cost = 28.167\n",
      "--- 16.607133150100708 seconds ---\n",
      "Accuracy: 0.8548999983072281\n",
      "Epoch: 0176 Avg. cost = 28.209\n",
      "--- 16.7081880569458 seconds ---\n",
      "Accuracy: 0.8479999995231629\n",
      "Epoch: 0177 Avg. cost = 27.795\n",
      "--- 16.7291522026062 seconds ---\n",
      "Accuracy: 0.8593000006675721\n",
      "Epoch: 0178 Avg. cost = 27.681\n",
      "--- 16.803000450134277 seconds ---\n",
      "Accuracy: 0.855100000500679\n",
      "Epoch: 0179 Avg. cost = 27.594\n",
      "--- 16.703131914138794 seconds ---\n",
      "Accuracy: 0.8498000031709672\n",
      "Epoch: 0180 Avg. cost = 26.851\n",
      "--- 16.779363870620728 seconds ---\n",
      "Accuracy: 0.856599998474121\n",
      "Epoch: 0181 Avg. cost = 26.493\n",
      "--- 16.7521870136261 seconds ---\n",
      "Accuracy: 0.8473999983072281\n",
      "Epoch: 0182 Avg. cost = 26.822\n",
      "--- 16.72115921974182 seconds ---\n",
      "Accuracy: 0.8526999998092651\n",
      "Epoch: 0183 Avg. cost = 28.370\n",
      "--- 16.696184635162354 seconds ---\n",
      "Accuracy: 0.8523000037670135\n",
      "Epoch: 0184 Avg. cost = 25.918\n",
      "--- 16.882225513458252 seconds ---\n",
      "Accuracy: 0.8581999981403351\n",
      "Epoch: 0185 Avg. cost = 27.827\n",
      "--- 16.818998336791992 seconds ---\n",
      "Accuracy: 0.8534000009298325\n",
      "Epoch: 0186 Avg. cost = 26.621\n",
      "--- 16.80126929283142 seconds ---\n",
      "Accuracy: 0.8591999989748001\n",
      "Epoch: 0187 Avg. cost = 26.880\n",
      "--- 16.71808671951294 seconds ---\n",
      "Accuracy: 0.8519999974966049\n",
      "Epoch: 0188 Avg. cost = 28.504\n",
      "--- 16.792022943496704 seconds ---\n",
      "Accuracy: 0.8519999980926514\n",
      "Epoch: 0189 Avg. cost = 26.957\n",
      "--- 16.708397150039673 seconds ---\n",
      "Accuracy: 0.8457999980449676\n",
      "Epoch: 0190 Avg. cost = 26.491\n",
      "--- 16.658077001571655 seconds ---\n",
      "Accuracy: 0.8467999988794327\n",
      "Epoch: 0191 Avg. cost = 26.026\n",
      "--- 16.79003405570984 seconds ---\n",
      "Accuracy: 0.8560999989509582\n",
      "Epoch: 0192 Avg. cost = 26.116\n",
      "--- 16.71613311767578 seconds ---\n",
      "Accuracy: 0.8558999967575073\n",
      "Epoch: 0193 Avg. cost = 26.265\n",
      "--- 16.688102960586548 seconds ---\n",
      "Accuracy: 0.8606000006198883\n",
      "Epoch: 0194 Avg. cost = 25.976\n",
      "--- 16.810261964797974 seconds ---\n",
      "Accuracy: 0.8612999975681305\n",
      "Epoch: 0195 Avg. cost = 25.651\n",
      "--- 16.779178380966187 seconds ---\n",
      "Accuracy: 0.8546999961137771\n",
      "Epoch: 0196 Avg. cost = 25.854\n",
      "--- 16.783165216445923 seconds ---\n",
      "Accuracy: 0.8541000026464463\n",
      "Epoch: 0197 Avg. cost = 25.887\n",
      "--- 16.820031881332397 seconds ---\n",
      "Accuracy: 0.8577999985218048\n",
      "Epoch: 0198 Avg. cost = 25.453\n",
      "--- 16.71619415283203 seconds ---\n",
      "Accuracy: 0.8506999987363816\n",
      "Epoch: 0199 Avg. cost = 24.798\n",
      "--- 16.721142053604126 seconds ---\n",
      "Accuracy: 0.8539999967813492\n",
      "Epoch: 0200 Avg. cost = 24.648\n",
      "--- 16.751028537750244 seconds ---\n",
      "Accuracy: 0.853400000333786\n",
      "Epoch: 0201 Avg. cost = 25.673\n",
      "--- 16.68897819519043 seconds ---\n",
      "Accuracy: 0.8501999950408936\n",
      "Epoch: 0202 Avg. cost = 24.023\n",
      "--- 16.680089235305786 seconds ---\n",
      "Accuracy: 0.8542999994754791\n",
      "Epoch: 0203 Avg. cost = 26.232\n",
      "--- 16.769015550613403 seconds ---\n",
      "Accuracy: 0.85\n",
      "Epoch: 0204 Avg. cost = 25.574\n",
      "--- 16.71339201927185 seconds ---\n",
      "Accuracy: 0.8516000014543533\n",
      "Epoch: 0205 Avg. cost = 24.713\n",
      "--- 16.718045473098755 seconds ---\n",
      "Accuracy: 0.8502999985218048\n",
      "Epoch: 0206 Avg. cost = 24.463\n",
      "--- 16.855178594589233 seconds ---\n",
      "Accuracy: 0.8523999977111817\n",
      "Epoch: 0207 Avg. cost = 24.913\n",
      "--- 16.775280952453613 seconds ---\n",
      "Accuracy: 0.8539999961853028\n",
      "Epoch: 0208 Avg. cost = 24.869\n",
      "--- 16.69806671142578 seconds ---\n",
      "Accuracy: 0.8542999976873398\n",
      "Epoch: 0209 Avg. cost = 24.592\n",
      "--- 16.82313084602356 seconds ---\n",
      "Accuracy: 0.8538999998569489\n",
      "Epoch: 0210 Avg. cost = 26.099\n",
      "--- 16.730382204055786 seconds ---\n",
      "Accuracy: 0.8643000006675721\n",
      "Epoch: 0211 Avg. cost = 23.268\n",
      "--- 16.67221188545227 seconds ---\n",
      "Accuracy: 0.8590000003576279\n",
      "Epoch: 0212 Avg. cost = 24.587\n",
      "--- 16.694998264312744 seconds ---\n",
      "Accuracy: 0.8537000018358231\n",
      "Epoch: 0213 Avg. cost = 25.684\n",
      "--- 16.805031061172485 seconds ---\n",
      "Accuracy: 0.8533999997377396\n",
      "Epoch: 0214 Avg. cost = 23.087\n",
      "--- 16.63522958755493 seconds ---\n",
      "Accuracy: 0.857100002169609\n",
      "Epoch: 0215 Avg. cost = 23.123\n",
      "--- 16.579174518585205 seconds ---\n",
      "Accuracy: 0.8529999989271164\n",
      "Epoch: 0216 Avg. cost = 26.051\n",
      "--- 16.60600256919861 seconds ---\n",
      "Accuracy: 0.8541999995708466\n",
      "Epoch: 0217 Avg. cost = 24.219\n",
      "--- 16.76400065422058 seconds ---\n",
      "Accuracy: 0.8541999995708466\n",
      "Epoch: 0218 Avg. cost = 23.492\n",
      "--- 16.777114868164062 seconds ---\n",
      "Accuracy: 0.8542999988794326\n",
      "Epoch: 0219 Avg. cost = 25.176\n",
      "--- 16.65706706047058 seconds ---\n",
      "Accuracy: 0.8579999965429306\n",
      "Epoch: 0220 Avg. cost = 22.363\n",
      "--- 16.44499683380127 seconds ---\n",
      "Accuracy: 0.8582000023126602\n",
      "Epoch: 0221 Avg. cost = 22.595\n",
      "--- 16.793198823928833 seconds ---\n",
      "Accuracy: 0.8605000007152558\n",
      "Epoch: 0222 Avg. cost = 22.217\n",
      "--- 16.5740966796875 seconds ---\n",
      "Accuracy: 0.8575\n",
      "Epoch: 0223 Avg. cost = 23.350\n",
      "--- 16.829267978668213 seconds ---\n",
      "Accuracy: 0.8499000030755997\n",
      "Epoch: 0224 Avg. cost = 22.440\n",
      "--- 16.618378400802612 seconds ---\n",
      "Accuracy: 0.8531999999284744\n",
      "Epoch: 0225 Avg. cost = 23.390\n",
      "--- 16.465043544769287 seconds ---\n",
      "Accuracy: 0.8640999978780747\n",
      "Epoch: 0226 Avg. cost = 24.013\n",
      "--- 16.292147874832153 seconds ---\n",
      "Accuracy: 0.8529999995231629\n",
      "Epoch: 0227 Avg. cost = 23.655\n",
      "--- 16.341004848480225 seconds ---\n",
      "Accuracy: 0.853199999332428\n",
      "Epoch: 0228 Avg. cost = 22.017\n",
      "--- 16.706274032592773 seconds ---\n",
      "Accuracy: 0.850599998831749\n",
      "Epoch: 0229 Avg. cost = 22.533\n",
      "--- 16.749215126037598 seconds ---\n",
      "Accuracy: 0.8524999976158142\n",
      "Epoch: 0230 Avg. cost = 22.339\n",
      "--- 16.734039068222046 seconds ---\n",
      "Accuracy: 0.854200000166893\n",
      "Epoch: 0231 Avg. cost = 22.284\n",
      "--- 16.813286542892456 seconds ---\n",
      "Accuracy: 0.8558999973535538\n",
      "Epoch: 0232 Avg. cost = 22.960\n",
      "--- 16.601073265075684 seconds ---\n",
      "Accuracy: 0.8537999975681305\n",
      "Epoch: 0233 Avg. cost = 24.090\n",
      "--- 16.667067766189575 seconds ---\n",
      "Accuracy: 0.8502000010013581\n",
      "Epoch: 0234 Avg. cost = 23.161\n",
      "--- 16.81495237350464 seconds ---\n",
      "Accuracy: 0.853199999332428\n",
      "Epoch: 0235 Avg. cost = 23.170\n",
      "--- 16.768118381500244 seconds ---\n",
      "Accuracy: 0.8560999989509582\n",
      "Epoch: 0236 Avg. cost = 22.710\n",
      "--- 16.763183116912842 seconds ---\n",
      "Accuracy: 0.8604999989271164\n",
      "Epoch: 0237 Avg. cost = 23.320\n",
      "--- 16.724209785461426 seconds ---\n",
      "Accuracy: 0.8614999955892563\n",
      "Epoch: 0238 Avg. cost = 22.449\n",
      "--- 16.616000413894653 seconds ---\n",
      "Accuracy: 0.8533000004291534\n",
      "Epoch: 0239 Avg. cost = 22.419\n",
      "--- 16.774090051651 seconds ---\n",
      "Accuracy: 0.856199996471405\n",
      "Epoch: 0240 Avg. cost = 21.976\n",
      "--- 16.79623556137085 seconds ---\n",
      "Accuracy: 0.8573000001907348\n",
      "Epoch: 0241 Avg. cost = 22.929\n",
      "--- 16.793067693710327 seconds ---\n",
      "Accuracy: 0.8551999998092651\n",
      "Epoch: 0242 Avg. cost = 23.132\n",
      "--- 16.793256044387817 seconds ---\n",
      "Accuracy: 0.8502999955415725\n",
      "Epoch: 0243 Avg. cost = 22.794\n",
      "--- 16.81040120124817 seconds ---\n",
      "Accuracy: 0.8518999969959259\n",
      "Epoch: 0244 Avg. cost = 21.779\n",
      "--- 16.710160493850708 seconds ---\n",
      "Accuracy: 0.8544999986886979\n",
      "Epoch: 0245 Avg. cost = 21.028\n",
      "--- 16.8210232257843 seconds ---\n",
      "Accuracy: 0.855\n",
      "Epoch: 0246 Avg. cost = 21.690\n",
      "--- 16.878239393234253 seconds ---\n",
      "Accuracy: 0.8588999992609024\n",
      "Epoch: 0247 Avg. cost = 22.104\n",
      "--- 16.564260005950928 seconds ---\n",
      "Accuracy: 0.8606000024080277\n",
      "Epoch: 0248 Avg. cost = 20.038\n",
      "--- 16.71025276184082 seconds ---\n",
      "Accuracy: 0.8497999995946884\n",
      "Epoch: 0249 Avg. cost = 22.805\n",
      "--- 16.584136962890625 seconds ---\n",
      "Accuracy: 0.8553999990224839\n",
      "Epoch: 0250 Avg. cost = 22.854\n",
      "--- 16.735079050064087 seconds ---\n",
      "Accuracy: 0.8506000000238418\n",
      "End of optimization!\n"
     ]
    }
   ],
   "source": [
    "# Loss 함수\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_))\n",
    "\n",
    "#cross_entropy = -tf.reduce_sum(y_*tf.log(y))\n",
    "#cross_entropy = tf.reduce_mean((y-y_)**2)\n",
    "train_batch_size = 100\n",
    "test_batch_size = 100\n",
    "total_train_batch = int(50000/train_batch_size)\n",
    "total_test_batch = int(10000/test_batch_size)\n",
    "\n",
    "#checkpoint를 저장하는 위치\n",
    "#ckpt = tf.train.get_checkpoint_state('./cnn(cifar10)')\n",
    "# global_variables 함수를 통해 앞서 정의하였던 변수들을 저장하거나 불러올 변수들로 설정함.\n",
    "\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "\n",
    "number_of_epochs = 250\n",
    "#batch normalization을 제대로 하기 위해 graph에 mean average를 update하라고 말한다.\n",
    "# http://ruishu.io/2016/12/27/batchnorm/ 를 참조하기 \n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "\n",
    "with tf.control_dependencies(update_ops):\n",
    "     # Ensures that we execute the update_ops before performing the train_step\n",
    "     train_step = tf.train.AdamOptimizer(0.001).minimize(cost, global_step=global_step)\n",
    "    \n",
    "gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "sess = tf.InteractiveSession(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "\n",
    "#sess = tf.InteractiveSession() \n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for epoch in range(number_of_epochs):\n",
    "    total_cost = 0\n",
    "    total_accuracy = 0\n",
    "    #checkpoint가 있으면 restore하고 아니면 변수들을 초기화한다. \n",
    "#    if ckpt:\n",
    "#        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "#    else:\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "    start_time = time.time()\n",
    "    for i in range(total_train_batch):\n",
    "        \n",
    "        batch = next_batch(train_batch_size, x_train, y_train_one_hot.eval())\n",
    "        _, cost_val = sess.run([train_step,cost], feed_dict={x: batch[0], y_: batch[1], phase: True})\n",
    "        total_cost += cost_val\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1),'Avg. cost =', '{:.3f}'.format(total_cost))\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "        \n",
    "    for i in range(total_test_batch):\n",
    "\n",
    "        test_batch = next_batch(test_batch_size, x_test, y_test_one_hot.eval()) \n",
    "        acc = sess.run(accuracy, feed_dict={x: test_batch[0], y_: test_batch[1], phase: False})\n",
    "        total_accuracy += acc\n",
    "    \n",
    "    mean_accuracy = total_accuracy/total_test_batch\n",
    "    print('Accuracy:', mean_accuracy) # test batch에 대한 정확도 출력....(batch사이즈가 작아서 정확하지 않아도 됨.)\n",
    "    \n",
    "#변수를 checkpoint파일에 저장.    \n",
    "#    saver.save(sess, './cnn(cifar10)/dnn.ckpt', global_step=global_step)\n",
    "\n",
    "\n",
    "print('End of optimization!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
